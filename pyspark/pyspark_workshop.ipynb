{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3443a24b",
   "metadata": {},
   "source": [
    "Pokud pracujete s dvojkovým PySparkem, zakomentujte \"UnivariateFeatureSelector\"  a \"from pyspark.ml.functions import vector_to_array\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57383bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import ChiSqSelector, UnivariateFeatureSelector\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "from pyspark.ml.functions import vector_to_array \n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357b2e5",
   "metadata": {},
   "source": [
    "# PySpark\n",
    "\n",
    "## Obsah\n",
    "- [Co je Spark a PySpark](#Co-je-Spark-a-PySpark)\n",
    "- [Instalace](#Instalace)\n",
    "- [PySpark sešny](#PySpark-sešny)\n",
    "- [Práce s daty](#Práce-s-daty)\n",
    "  - [Výroba tabulkoidního objektu](#Výroba-tabulkoidního-objektu) \n",
    "  - [Dataframový přístup](#Dataframový-přístup)\n",
    "    - [Informační metody](#Informační-metody)\n",
    "    - [Manipulace se sloupci](#Manipulace-se-sloupci)\n",
    "    - [PySpark a datumy](#PySpark-a-datumy)\n",
    "    - [Window funkce](#Window-funkce)\n",
    "  - [SQL přístup](#SQL-přístup)\n",
    "  - [Cachování](#Cachování)\n",
    "- [Machine learning](#Machine-learning)\n",
    "  - [String indexer](#String-indexer)\n",
    "  - [Vektorizace](#Vektorizace)\n",
    "  - [Scaling](#Scaling)\n",
    "  - [Trénování modelu](#Trénování-modelu)\n",
    "  - [Vyhodnocení přesnosti modelu](#Vyhodnocení-přesnosti-modelu)\n",
    "  - [Feature selection](#Feature-selection)\n",
    "  - [Gridsearch](#Gridsearch)\n",
    "  - [Pipelina](#Pipelina)\n",
    "  - [Ukládání modelu](#Ukládání-modelu)\n",
    "  - [Oversampling v PySparku](#Oversampling-v-PySparku)\n",
    "  - [Bucketování](#Bucketování)\n",
    "  - [Imputing](#Imputing)\n",
    "  - [One-hot encoding](#One-hot-encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2ddb4",
   "metadata": {},
   "source": [
    "## Co je Spark a PySpark\n",
    "Jeden počítač sice z hlediska výpočetní síly určitou práci zastane, více počítačů najednou je ale mocnějších. Přitom zde nemluvíme pouze o nějakých složitých algoritmech - tento poznatek je možná ještě očividnější při \"tupém\" procházení velkého množství dat řádek po řádku. Nicméně hromada počítačů/serverů (alias cluster) sama o sobě tolik nezmůže - musí být ošetřena komunikace a rozdělování práce mezi nimi. Tuto úlohu může krom jiných zastávat právě Spark. Alternativně lze cluster počítačů řídit nečím jiným, třeba [YARNem](https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html), přičemž na tuto věc se Spark napojí.  \n",
    "Aplikace běžící ve Sparku se skládá z jednoho driver procesu a několika executor procesů. Driver proces má za úkol spravovat informace o sparkovské aplikaci, komunikovat s uživatelem a řídit a distribuovat práci executor procesů. Executory pak vykonávají práci jim zadanou a výsledky svých výpočtů vrací na driver. V případě našeho povídání, kdy si vše budeme ukazovat na lokálu, budou driver i executory de facto obyčejné procesy. V reálném prostředí by pravda šlo též o obyčejné procesy, ty by ale běžely na odlišných mašinách.  \n",
    "Spark je napsán ve Scale, což není úplně nejsnazší jazyk na pochopení. Nicméně byla by škoda o sílu tohoto frameworku přijít. Proto vznikl PySpark jako svého druhu pythoní obal nad Sparkem. Z hodně naivního pohledu připomíná PySpark Pandas (balíček pro práci s tabulkami) zkřížený s balíčkem Scikit-learn (balíček pro strojové učení). Velký rozdíl ale tkví ve způsobu vyhodnocování příkazů. V Pandách je vyhodnocování \"eager\" (asi nemá cenu tento termín překládat - výsledek by čtenáře jen mátl). To znamená, že když spustíme jakoukoli pandí funkci či metodu, provede se okamžitě. Oproti tomu PySpark je lazy. Některé příkazy - tzv. transformace - se při odklepnutí v IDE fakticky nespustí, pouze se přidají do grafu vyhodnocování transformací. Tento graf, resp. operace v něm, odstartuje až když je to potřeba, obvykle kvůli příkazům typu akce. Příkladem transformace mohou být všelikaké úpravy pysparkového dataframu, zatímco příkladem akce by bylo zobrazení tohoto dataframu uživateli. Důvod takovéhoto chování je redukce míry načítání obřích dataframů do paměti a zefektivnění práce s clustery serverů. Člověk musí mít toto chování pořád na paměti, jinak zjistí, že mu aplikace padá. Proč? Jelikož samotné výpočty (obvykle řetěz transformací s obříma datama) probíhá na executor procesech, driver procesy nemají alokovány tolik paměti. Pakliže ale na tyto velká data vypustíme nevhodnou akci (třeba akci \"vytiskni celou obří tabulku\"), tak všechny executory svá data pošlou na driver a ten to nerozchodí.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb774bf",
   "metadata": {},
   "source": [
    "## Instalace\n",
    "V praxi Pyspark buďto bydlí na Linuxu a je napojen na HDFS, anebo ho uživatel najde v cloudu, dejme tomu v Azure jako Databrics. Nicméně první by pro potřeby tohoto výukového textu znamenalo složitejší instalaci, druhé zase vytažení platební karty. Proto si zde povíme, jak PySpark nainstalovat na obyčejný počítač, kde jako operační systém slouží MS Windows.  \n",
    "Při instalaci jsem postupoval podle tohoto [návodu](https://sparkbyexamples.com/pyspark/install-pyspark-in-anaconda-jupyter-notebook/). Pokud by odkaz nebyl v budoucnu funkční, shrnu zde hlavní body:  \n",
    "- instalace [Anacondy](https://www.anaconda.com/)  \n",
    "- instalace Javy (tu jsem měl nainstalovanou již dříve, nicméně podle návodu by to šlo udělat i přes anacondí repozitář pomocí \"conda install openjdk\")  \n",
    "- instalace PySparku pomocí \"conda install pyspark\"  \n",
    "- instalace balíčku findspark  \n",
    "\n",
    "V mém případě toto bohužel nestačilo. Nejprve se při otestování funkčnosti (napsání \"pyspark\" do anacondí konzole) objevila hláška na způsob\n",
    "```\n",
    "Python was not found; run without arguments to install from the Microsoft Store\n",
    "```\n",
    "Bylo třeba klepnout na Start a psát (přesněji začít psát) \"Manage app execution aliases\". Po kliknutí na odpovídající položku ve Startu se v okně, které se objevilo, musely vypnout všechny věci vypadající, že mají nějakou spojitost s Pythonem.  \n",
    "Při následovném pokusu o spuštění PySparku se v konzoli vypsala zpráva\n",
    "```\n",
    "Missing Python executable 'python3', defaulting to 'C:\\Users\\ThereWouldBeUserName\\miniconda3\\Scripts\\..' for SPARK_HOME environment variable. Please install Python or specify the correct Python executable in PYSPARK_DRIVER_PYTHON or PYSPARK_PYTHON environment variable to detect SPARK_HOME safely.\n",
    "```\n",
    "Řešení spočívalo v nastavení proměnné PYSPARK_PYTHON. To se realizuje následujícím příkazem napsaným do konzole\n",
    "```\n",
    "set PYSPARK_PYTHON=python\n",
    "```\n",
    "Zdůrazněme, že takto nastavená proměnná existuje jen do té doby, dokud není konzole zavřena. Pokud bychom na tento krok zapomenuli a spustili Jupyter Notebook, projeví se to v nekonečném vytváření sparkové sešny (obvykle první věc, kterou budeme dělat po importování balíčků). Nicméně v konzoli, ve které jsme Jupyter pustili, by se objevila výše uvedená \"missing python executable\" chyba.  \n",
    "Zmiňme nakonec, že v konzoli jak při použití Jupyteru, tak při spuštění PySpaarku napřímo uvidíme chybu spojenou s hadoopem:\n",
    "```\n",
    "java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\n",
    "```\n",
    "Ta naši práci neovlivní (žádný Hadoop u sebe nainstalovaný nemáme), takže ji můžeme ignorovat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6542fb87",
   "metadata": {},
   "source": [
    "## PySpark sešny\n",
    "Pro komunikaci se sparkovským enginem, bez kterého se neobejdeme, je třeba vytvořit sparkovskou sešnu. To provedeme následujícím kódem (vnější sada závorek tu je jen proto, abychom mohli provádět odřádkování a neměli tak jeden nekonečný řádek):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22be7906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"toy_application\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a080e",
   "metadata": {},
   "source": [
    "V tomto minimálním příkladu jsme sešnu pojmenovali (skrze *appName*) a poté jsme zavolali metodu *getOrCreate*. Ta zajistí, že šesna bude singleton. Tj. pokud sešna neexistuje, bude vytvořena, ale pokud už nějaká sešna předtím vznikla, *getOrCreate* vrátí právě tu. Toto chování si demonstrujme na zobrazení spark objektu a na vypsání jeho IDčka.  \n",
    "Když vložíme do buňky samostojící jméno sešny, dostaneme odkaz na Spark UI - de facto webový inteface, ve kterém můžeme běh sešny kontrolovat. Pod labelem \"Version\" se skrývá verze Sparku, \"Master\" nám zase řekne, kde vlastně Spark bydlí. Důležité pro nás v tomto bodě je \"AppName\", kde vidíme jméno naší aplikace - toy_application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17f21acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>toy_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1aef2e9aeb0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb04d55",
   "metadata": {},
   "source": [
    "Co se Spark UI týče, tak v záložce \"Jobs\" nalezneme obecné informace o sparkovské aplikaci a timelime graf. Ve \"stages\" se dá proklikat na DAGy (directed acyclic graph) reprezentující operace na sparkovských objektech a jsou zde i podrobné informace o tom, co a kdy se v rámci určité operace vlastně dělo. Ve storage záložce uvidíme, co vše bylo nacachované (viz kapitola v cca polovině výkladu - ale vidět tu je pouze sql cachování, nikoli \"normální\" pythoní cachování). Enviroments záložka obdahuje informace o všelijakých systémových proměnných. V executors záložce můžeme nalézt informace o executorech (co který executor dělá, kolik má k dispozici corů, kolik má reálně paměti apod.). Nakonec v sql záložce uvidíme exekuční plán sql dotazů."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2602efb7",
   "metadata": {},
   "source": [
    "Vypišme si idčko spark objektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c4525b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392617635216"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f351a2",
   "metadata": {},
   "source": [
    "Když se nyní pokusíme vytvořit novou sparkovsou sešnu, můžeme se přesvědčit, že nám *getOrCreate* opravdu vrátil sešnu původní."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4113550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392617635216"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_2 = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "id(spark_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3107bd",
   "metadata": {},
   "source": [
    "Dokonce ani snaha o nastavení odlišného appNamu nepovede k vytvoření opravdu nové sešny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3caf020d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392617635216"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_3 = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"another_toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "id(spark_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f267840",
   "metadata": {},
   "source": [
    "Vidíme, že nové jméno ani nedokázalo přepsat jméno staré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "035556e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>toy_application</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1443e7abd90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d119b10",
   "metadata": {},
   "source": [
    "Chceme-li z nějakého důvodu vytvořit sešnu novou, musíme na sešně staré zavolat *newSession*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183bae48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1392616115504"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_4 = spark.newSession()\n",
    "id(spark_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befb526f",
   "metadata": {},
   "source": [
    "Nicméně takto vytvořená sešna není úplně oddělená od sešny původní. Reálně se hodí jen pokud bychom opravdu potřebovali mít dva od sebe oddělené namespacy, tj. pokud bychom potřebovali, aby se pod stejným jménem ve dvou sešnách nacházely jiné objekty. Koneckonců i v dokumentaci se o *newSession* píše, že \n",
    "```\n",
    "Returns a new SparkSession as new session, that has separate SQLConf,\n",
    "registered temporary views and UDFs, but shared SparkContext and\n",
    "table cache.\n",
    "```\n",
    "Zde se mluví o jakémsi SparkContextu. Pokud bychom na internetu narazili na staré tutoriály (před Spark 2.0), viděli bychom, že se v nich SparkContext používá jako vstup do sparkovského enginu namísto SparkSession. Jaký je mezi těmito entitami rozdíl? SparkContext se musí složitěji (a explicitněji) nastavovat a na celém JVM (Java Virtual Machine alias to, na čem jede Spark) může existovat jen jeden. Nicméně i když ho v našem uživatelském kódu nikde nevytváříme, v pozadí pořád existuje. Když například chceme sešnu ukončit a provoláme na ní metodu *stop*, zastavíme i všechny ostatní sešny - koneckonců zkume si poté, co jsme ukončili sešnu spark_4, otevřít odkazy na Spark UI z předešlých sešen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c1b41f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_4.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fb47f",
   "metadata": {},
   "source": [
    "Docstring v metodě *stop* totiž zmiňuje právě SparkContext:\n",
    "```\n",
    "Stop the underlying :class:`SparkContext`.\n",
    "```\n",
    "Jinak co se týče zastavování sešen, v našem případě, kdy pracujeme na lokálu, to potřeba není. Celý Spark se totiž vypne s vypnutím Jupyteru. Nicméně na normálním prostředí už by zastavování sešen po skončení práce bylo životně důležité, aby se neblokovaly hardwarové prostředky.  \n",
    "Tyto hardwarové prostředky, které bude naše aplikace používat, bychom mohli specifikovat při vytváření sešny explicitně\n",
    "```\n",
    "spark_example = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.executors.cores\", \"5\")\n",
    "    .config(\"spark.driver.cores\", \"1\")\n",
    "    .config(\"spark.executor.memory\", \"4G\")\n",
    "    .config(\"spark.driver.memory\",\"2G\")\n",
    "    .appName(\"another_toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "```\n",
    "Možností pro nastavení je vcelku mnoho. Přitom zvolené hodnoty častokrát záleží na hardwarovém vybavení clusteru a na úloze, kterou je třeba řešit. Příklad nastavení pro různá předpokládána zatížení se nalézá v následující tabulce. Co který parametr dělá lze najít v [dokumentaci](https://spark.apache.org/docs/latest/configuration.html).\n",
    "\n",
    "| Parametr | Nízké zatížení | Střední zatížení | Vysoké zatížení | \n",
    "| --- | --- | --- | --- |  \n",
    "|spark.driver.memory|2G|3G|4G|  \n",
    "|spark.driver.memoryOverhead|256M|512M|1G|\n",
    "|spark.dynamicAllocation.enabled|true|true|true|\n",
    "|spark.dynamicAllocation.maxExecutors|10|20|50|\n",
    "|spark.executor.memory|4G|8G|12G|\n",
    "|spark.executor.memoryOverhead|512M|1G|2G|\n",
    "|spark.sql.shuffle.partitions|40|80|200|\n",
    "\n",
    "Zmiňme matoucí rozdíl mezi *executors.cores* a *executor.instances*. *Executers/driver.cores* říká, kolik má být v rámci jednoho executoru či drivero alokováno jader. Oproti tomu *executor.instances* řídí, kolik vůbec executorů bude. \n",
    "\n",
    "Na každý pád bude spíše než ruční nastavování prostředků častější situace, kdy pro nás někdo z infrastruktury vytvoří uživatelský pool s dedikovanými prostředky, do kterého se přihlásíme variací na\n",
    "```\n",
    "spark_example = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.yaml.queue\", \"pool_data_scientist\")\n",
    "    .appName(\"another_toy_application\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b064993f",
   "metadata": {},
   "source": [
    "## Práce s daty\n",
    "Základní stavební jednotkou dat, se kterým Spark pracuje, je tzv. Resilient Distributed Dataset (RDD). Můžeme si ho představit jako svého druhu tuple, na kterém lze efektivně práci paralelizovat. Zdůrazněme, že RDD má blíž k tuplu než k listu - je immutable, tj. operace na něm proběhlá ho nemění, nýbrž vrací zeditovanou kopii. Na každý pád dnes není moc důvodů s RDD napřímo pracovat. Vznikl totiž datový objekt, se kterým se manipuluje snadněji - DataFrame. Jméno připomíná základní stavební jednotku balíčku Pandas a podobnost není čistě náhodná - opravdu se jedná o reprezentaci tabulkoidního objektu. Naše povídání se tak bude celou dobu a zejména v této kapitole týkat právě tohoto typu objektu. Pro úplnost ještě poznamenejme, že ve Sparku existují tzv. Datasety, které ale nejsou v PySparku implementovány.\n",
    "\n",
    "### Výroba tabulkoidního objektu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70e984a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"toy_application\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc067c1b",
   "metadata": {},
   "source": [
    "Asi nejpřímočařejším, byť ne úplně využívaným postupem na vytvoření pysparkového dataframu, je jeho konstrukce z dat zadaných v programu v podobě listu. Tento list se skládá z podlistů či z tuplů. Ty v budoucí tabulce budou reprezentovat jednotlivé řádky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d97971",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in_list = [\n",
    "    [\"Švejk\",40,\"Praha\"], \n",
    "    [\"Vyskočil\",50,\"Horní Dolní\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f515358",
   "metadata": {},
   "source": [
    "List vložíme jako parametr do funkce *createDataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72081b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_list = spark.createDataFrame(data_in_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e36432c",
   "metadata": {},
   "source": [
    "Výsledný dataframe si zobrazíme pomocí metody *show* (a ano, *show* je akce). Všimněme si, že jelikož jsme nespecifikovali hlavičku tabulky, objevila se nám posloupnost čísel \\_1, \\_2 atd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba541cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|      _1| _2|         _3|\n",
      "+--------+---+-----------+\n",
      "|   Švejk| 40|      Praha|\n",
      "|Vyskočil| 50|Horní Dolní|\n",
      "+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9d2fed",
   "metadata": {},
   "source": [
    "Pro vytvoření dataframu s hlavičkou existuje v *createDataFrame* speciální parametr - *schema* s listem obsahujícím jména sloupců."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1942bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|    name|age|       city|\n",
      "+--------+---+-----------+\n",
      "|   Švejk| 40|      Praha|\n",
      "|Vyskočil| 50|Horní Dolní|\n",
      "+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_list = spark.createDataFrame(\n",
    "    data_in_list,\n",
    "    schema=[\"name\", \"age\", \"city\"]\n",
    ")\n",
    "table_from_list.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea0a87",
   "metadata": {},
   "source": [
    "Pysparkový dataframe lze vytvořit i z pandího dataframu - zkrátka se do *createDataFrame* namísto listu vloží právě onen pandí dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22de6afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----------+\n",
      "|    name|age|       city|\n",
      "+--------+---+-----------+\n",
      "|   Švejk| 40|      Praha|\n",
      "|Vyskočil| 50|Horní Dolní|\n",
      "+--------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_dataframe = pd.DataFrame({\n",
    "    \"name\": [\"Švejk\", \"Vyskočil\"],\n",
    "    \"age\": [40, 50],\n",
    "    \"city\": [\"Praha\", \"Horní Dolní\"]\n",
    "})\n",
    "table_from_pandas = spark.createDataFrame(pandas_dataframe)\n",
    "table_from_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf5ef22",
   "metadata": {},
   "source": [
    "Pokud v *createDataFrame* uvedeme jména sloupců, mají tyto jména oproti těm v pandím dataframu prioritu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b5d0b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+\n",
      "|last_name|age|    address|\n",
      "+---------+---+-----------+\n",
      "|    Švejk| 40|      Praha|\n",
      "| Vyskočil| 50|Horní Dolní|\n",
      "+---------+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pandas_dataframe = pd.DataFrame({\n",
    "    \"name\": [\"Švejk\", \"Vyskočil\"],\n",
    "    \"age\": [40, 50],\n",
    "    \"city\": [\"Praha\", \"Horní Dolní\"]\n",
    "})\n",
    "table_from_pandas = spark.createDataFrame(\n",
    "    pandas_dataframe,\n",
    "    schema=[\"last_name\", \"age\", \"address\"]\n",
    ")\n",
    "table_from_pandas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bea49f",
   "metadata": {},
   "source": [
    "Pro úplnost zmiňme, že pokud bychom potřebovali konverzi opačným směrem, tj. z PySparku do Pand, použijeme metodu *toPandas*. Jenže bacha - když to uděláme, půjdou všechna data na driver, který to v případě velké tabulky nerozdýchá. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94d06ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_name</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Švejk</td>\n",
       "      <td>40</td>\n",
       "      <td>Praha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vyskočil</td>\n",
       "      <td>50</td>\n",
       "      <td>Horní Dolní</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  last_name  age      address\n",
       "0     Švejk   40        Praha\n",
       "1  Vyskočil   50  Horní Dolní"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pandas_frame = table_from_pandas.toPandas()\n",
    "new_pandas_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb08ca6",
   "metadata": {},
   "source": [
    "Načíst data jde pochopitelně i z csv souboru. Na to slouží *read.csv*. Parametry jsou obdobné jaké v pandím *read_csv*. Zdůrazněme ale, že v parametru *path* specifikujícím cestu k souboru musí být string (resp. list stringů), nikoli *pathlib.Path*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19149ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = spark.read.csv(path=\"pyspark_data/csv/data_for_csv_load.csv\", sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300e9710",
   "metadata": {},
   "source": [
    "Takto to vypadá neproblematicky. Nicméně zkusme na náš dataframe vypustit metodu *printSchema*, který ukáže datové typy sloupců. Uvidíme, že jsou všechny brány jako stringy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa20a9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b63cc2",
   "metadata": {},
   "source": [
    "Takové je chování, které nastává při nastavení parametru *inferSchema* na hodnotu None resp. False. Co dělat, kdybychom chtěli mít rozumné datové typy už od načtení? Když se *inferSchema* položí rovno True, PySpark načte prvních pár řádků ze souboru a z nich datový typ odvodí. Problém nastane v okamžiku, kdy se první řádky liší od řádků následujících a je tak automaticky zvolen nevhodný datový typ. Anebo se \"zbytečně\" kvůli typům načte celý soubor, což může trvat docela dlouho. Proto bývá vhodné definovat datový typ explicitně. Na to použijeme objekt typu *StructType* reprezentující schéma celého dataframu. Tento *StructType* obsahuje objekty *StructField* obsahující informace o jednotlivých sloupcích. První parametr konstruktoru *StructField* obsahuje jméno sloupce, druhý parametr datový typ. Třetí parametr by měl dle [dokumentace](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.types.StructField.html) obdsahovat informaci, zda pole může být nullable. Nicméně při mých pokusech na Sparku 3 se tento atribut po vytvoření dataframu nezávisle na tom, co jsem do konstuktoru vložil, automaticky nastavil na True (zjištěno prostřednictvím *table_from_csv.schema*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84e1b0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "file_table_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", DoubleType()),\n",
    "    StructField(\"gender\", StringType())\n",
    "])\n",
    "\n",
    "table_from_csv = spark.read.csv(\n",
    "    path=\"pyspark_data/csv/data_for_csv_load.csv\", sep=\",\", \n",
    "    header=True, schema=file_table_schema\n",
    ")\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebd68e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be9b8d1",
   "metadata": {},
   "source": [
    "Když nějaký sloupec dostane špatný datový typ (například \"age\" vs souboru je ve fromátu 25.0, tj. je to float, ale my mu dáme IntegerType), celý sloupec se naplnít nully (u dvojkového Sparku se nully objevily ve *všech* sloupcích)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5bb592b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|null|     M|\n",
      "|200|      Mary|    Shelly|null|     F|\n",
      "|300|    Johann|    Geothe|null|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|null|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_table_schema = StructType([\n",
    "    StructField(\"id\", IntegerType()),\n",
    "    StructField(\"first_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType()),\n",
    "    StructField(\"age\", IntegerType()),\n",
    "    StructField(\"gender\", StringType())\n",
    "])\n",
    "\n",
    "table_from_csv = spark.read.csv(\n",
    "    path=\"pyspark_data/csv/data_for_csv_load.csv\", \n",
    "    sep=\",\", header=True, schema=file_table_schema\n",
    ")\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c7ec09",
   "metadata": {},
   "source": [
    "Výše jsme zmínili, že při načítání souborů mužeme uvést nikoli pouze string, ale i list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57b4c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|160|     Isaac|    Asimov|48.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_files_list = [\n",
    "    \"pyspark_data/csv/data_for_csv_load.csv\", \n",
    "    \"pyspark_data/csv/data_for_csv_load_2.csv\"\n",
    "]\n",
    "table_from_csv = spark.read.csv(path=csv_files_list, sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c1f13c",
   "metadata": {},
   "source": [
    "Nicméně musíme si dát pozor na to, aby soubory měly stejná schémata. V opačném případě se použije schéma jednoho ze souborů. U dat ze souborů ostatních se neexistující sloupce doplní nully a nadbytečné sloupce se useknou. Je otázkou, co určuje schématu dominující soubor - pořadí v listu to není."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b5dbe22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|160|     Isaac|    Asimov|48.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csv_files_list = [\n",
    "    \"pyspark_data/csv/data_for_csv_load.csv\", \n",
    "    \"pyspark_data/csv/data_for_csv_load_3.csv\"\n",
    "]\n",
    "table_from_csv = spark.read.csv(path=csv_files_list, sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2b8276",
   "metadata": {},
   "source": [
    "Údajně by dokonce mělo stačit do pathy vložit adresář obsahující csv soubory a do dataframu by se měl obsah všech souborů načíst. Nicméně na lokálu se mi to nepodařilo. Na druhou stranu na HDFS tato featura funguje."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7abe12e",
   "metadata": {},
   "source": [
    "Uložení dataframu do csv souboru se provede pomocí\n",
    "```\n",
    "table_from_csv.write.csv(\"data_after_saving_to_csv.csv\", sep=\"|\")\n",
    "```\n",
    "Když se o to ale pokusíme na lokále na Windows, obdržíme chybovou hlášku mluvící o Hadoopu. Proto pokud chceme dataframe uložit na (normální) disk, bude asi nejsnazší ho napřed zkonvertovat do pandí verze pomocí *toPandas* a poté uložit pandí metodou *to_csv*.  \n",
    "\n",
    "Na HDFS je ale situace odlišná. Při proběhnutí ukládací metody dojde v našem konkrétním příkladě k tomu, že se vytvoří adresář \"data_after_saving_to_csv.csv\" (ano, včetně souborové přípony). V tomto adresáři nalezneme jednak prázdný soubor SUCCESS, jednak několikero part souborů. Počet těchto souborů odpovídá počtu partitions (data v souboru jsou ta, která byla v příslušné partition). A co že je to ta partition? Jedná se o výsek dat, který je zpracováván jedním nodem, přesněji jedním jádrem na nodu (tj. na nodu může bydlet jedna nebo více partition, avšak jedna partition nemůže být najednou na více nodech). Defaultně je počet partition roven právě počtu jader přes celý cluster nodů. Příliš málo partition vede k tomu, že některé stroje stojí a data jsou skewnutá (např. na jednom nodu je 50 záznamů a na druhém 50 milionů), příliš mnoho partitions zase znamená, že orchestrování celého výpočetního procesu zabere více zdrojů než proces sám.  \n",
    "\n",
    "Co dělat, pokud bychom chtěli mít jenom jeden soubor namísto několika? Nejdříve si musíme ujasnit, jestli to opravdu chceme. Mít data v jednom souboru znamená mít data na jednom nodu - a na něj se nemusí vejít. Pro změnu počtu partition lze použít buďto dataframovou metodu *repartition*, anebo metodu *coalesce*. *Repartition* se snaží popřesunovat data skrze síť (shuffling) tak, aby byly velikosti jednotlivých partition zhruba stejné. Přitom nový počet partitions může být větší i menší než počet původní. U *coalesce* dává smysl jen počet menší. *Coalesce* se totiž snaží přesunout pouze data, u kterých je to nutné. Pokud na nodu už data byla v nějaké partition původní a na onom nodu má být partition nová, data zůstávají. To samozřejmě může vést k tomu, že na některých nodech bude záznamů více než na jiných, tj. můžeme se setkat se skewem.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab3b36",
   "metadata": {},
   "source": [
    "Mějme json o obsahu\n",
    "```json\n",
    "{\"first_name\": \"John\", \"last_name\": \"Doe\"},\n",
    "{\"first_name\": \"Jane\", \"last_name\": \"Doe\"}\n",
    "```\n",
    "Do dataframu ho načteme příkazem *spark.read.json*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d58f7337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|      Doe|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(path=\"pyspark_data/json/data_for_json_load.json\")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ffa3f",
   "metadata": {},
   "source": [
    "Nicméně pokud budou záznamy roztaženy na více než jeden řádek, např. takto:\n",
    "```json\n",
    "{\n",
    "  \"first_name\": \"John\", \n",
    "  \"last_name\": \"Doe\"\n",
    "},\n",
    "{\n",
    "  \"first_name\": \"Jane\", \n",
    "  \"last_name\": \"Doe\"\n",
    "}\n",
    "```\n",
    "stejný kód nám spadne s chybovou hláškou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "071014af",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e4d4c6a839c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtable_from_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pyspark_data/json/data_for_json_load_multiline.json\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtable_from_json\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\newnotebook\\miniconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\newnotebook\\miniconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\newnotebook\\miniconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).json(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).json(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).json(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count()."
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(path=\"pyspark_data/json/data_for_json_load_multiline.json\")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375b2ad",
   "metadata": {},
   "source": [
    "Při pohledu do dokumentace by se zdálo, že řešením je použití parametru multiLine s hodnotou True. Jenže ouha, v takto vytvořené tabulce je jen jeden záznam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96c70226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(\n",
    "    path=\"pyspark_data/json/data_for_json_load_multiline.json\",\n",
    "    multiLine=True\n",
    ")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24bc75d",
   "metadata": {},
   "source": [
    "Aby Spark správně multiline jsony interpretoval, musí tyto začínat a končit hranatými závorkami, tj. musí vypadat takto:\n",
    "```json\n",
    "[{\n",
    "  \"first_name\": \"John\", \n",
    "  \"last_name\": \"Doe\"\n",
    "},\n",
    "{\n",
    "  \"first_name\": \"Jane\", \n",
    "  \"last_name\": \"Doe\"\n",
    "}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc5ce476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|first_name|last_name|\n",
      "+----------+---------+\n",
      "|      John|      Doe|\n",
      "|      Jane|      Doe|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(\n",
    "    path=\"pyspark_data/json/data_for_json_load_multiline_2.json\",\n",
    "    multiLine=True\n",
    ")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd70d2a",
   "metadata": {},
   "source": [
    "Hnízděné jsony lze načíst též, ale poté musí následovat zprocesování dataframu do použitelnější podoby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ba97e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+---------+\n",
      "|             address|first_name|last_name|\n",
      "+--------------------+----------+---------+\n",
      "|[{Prague, Some st...|      John|      Doe|\n",
      "|[{Brno, Some stre...|      Jane|      Doe|\n",
      "+--------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_json = spark.read.json(\n",
    "    path=\"pyspark_data/json/data_for_json_load_complicated.json\",\n",
    ")\n",
    "table_from_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f72ef05",
   "metadata": {},
   "source": [
    "Krátce zmiňme práci s parquet soubory. Parquet je columnar file storage, což fakticky znamená, že se u něj efektivně dají provádět operace na sloupcích. Například pokud bychom měli tabulku ve formátu csv a chtěli spočítat sumu jednoho sloupce, musely by se před oním výpočtem přečíst všechny řádky souboru. U parquetu postačí přečíst odpovídající jednolitou sekci parquet souboru.  \n",
    "Uložení do parquetu provedeme takto:\n",
    "```python\n",
    "table_to_parquet.write.parquet(\"some_directory_created_by_this_command\")\n",
    "```\n",
    "Zdůrazněme, že stejně jako výše u csvčka dává smysl použít *coalesce*/*repartition* na redukci počtu souborů - opět se totiž nevytvoří jeden soubor, ale adresář se souborem za každou partition.\n",
    "Data z parquetu načteme tímto způsobem:\n",
    "```python\n",
    "spark.read.parquet(\"some_directory\")\n",
    "```\n",
    "\n",
    "Nakonec pokud bychom chtěli dataframe uložit jako hivovsku tabulku, použijeme\n",
    "```python\n",
    "pyspark_table_to_hive_table.write.saveAsTable(\"hive_database_container.hive_table_name\")\n",
    "```\n",
    "Jen bacha na práva. Mohlo by se totiž stát, že takto vytvořenou tabulku nebudeme právě kvůli nim později schopní smazat..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d32ae",
   "metadata": {},
   "source": [
    "### Dataframový přístup\n",
    "#### Informační metody\n",
    "V této podkapitole se budeme věnovat operacím s dataframy. Víceméně nám půjde o to najít ekvivalenty k pandasím konstrukcím.  \n",
    "V předcházející podkapitole jsme viděli, že se dataframe (přesněji prvních pár řádků) dá zobrazit pomocí metody *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00266951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = spark.read.csv(path=\"pyspark_data/csv/data_for_csv_load.csv\", sep=\",\", header=True)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801bd12",
   "metadata": {},
   "source": [
    "V případě, že chceme vidět jen prvních N řádků, vložíme toto číslo N jako parametr do *show*. Ten bude tudíž ekvivalentní pandasímu *head*. Defaultní hodnota tohoto parametru nesoucího jméno *n* je 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33cff86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+\n",
      "| id|first_name|last_name| age|gender|\n",
      "+---+----------+---------+----+------+\n",
      "|100|    Victor|     Hugo|25.0|     M|\n",
      "|200|      Mary|   Shelly|30.0|     F|\n",
      "+---+----------+---------+----+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b1e12",
   "metadata": {},
   "source": [
    "Pakliže je délka řetězce v některém sloupci delší než 20 znaků, je z hlediska toho, co zobrazuje *show*, tento řetězec oříznut (reálně se ale obsah buňky neztrácí)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6a418e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+\n",
      "|numbers_1|numbers_2|           long_word|\n",
      "+---------+---------+--------------------+\n",
      "|        0|        0|                    |\n",
      "|        1|       10|                 abc|\n",
      "|        2|       20|              abcabc|\n",
      "|        3|       30|           abcabcabc|\n",
      "|        4|       40|        abcabcabcabc|\n",
      "|        5|       50|     abcabcabcabcabc|\n",
      "|        6|       60|  abcabcabcabcabcabc|\n",
      "|        7|       70|abcabcabcabcabcab...|\n",
      "|        8|       80|abcabcabcabcabcab...|\n",
      "|        9|       90|abcabcabcabcabcab...|\n",
      "|       10|      100|abcabcabcabcabcab...|\n",
      "|       11|      110|abcabcabcabcabcab...|\n",
      "|       12|      120|abcabcabcabcabcab...|\n",
      "|       13|      130|abcabcabcabcabcab...|\n",
      "|       14|      140|abcabcabcabcabcab...|\n",
      "|       15|      150|abcabcabcabcabcab...|\n",
      "|       16|      160|abcabcabcabcabcab...|\n",
      "|       17|      170|abcabcabcabcabcab...|\n",
      "|       18|      180|abcabcabcabcabcab...|\n",
      "|       19|      190|abcabcabcabcabcab...|\n",
      "+---------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_list = [\n",
    "    [number, number*10, \"abc\"*number] for number in range(100)\n",
    "]\n",
    "numbers_frame = spark.createDataFrame(\n",
    "    numbers_list,\n",
    "    schema=[\"numbers_1\", \"numbers_2\", \"long_word\"]\n",
    ")\n",
    "numbers_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f855cb4",
   "metadata": {},
   "source": [
    "Pokud ořezávání chceme vypnout, přidáme do *show* parametr *truncate* s hodnotou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1eced5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+---------------------------------------------------------+\n",
      "|numbers_1|numbers_2|long_word                                                |\n",
      "+---------+---------+---------------------------------------------------------+\n",
      "|0        |0        |                                                         |\n",
      "|1        |10       |abc                                                      |\n",
      "|2        |20       |abcabc                                                   |\n",
      "|3        |30       |abcabcabc                                                |\n",
      "|4        |40       |abcabcabcabc                                             |\n",
      "|5        |50       |abcabcabcabcabc                                          |\n",
      "|6        |60       |abcabcabcabcabcabc                                       |\n",
      "|7        |70       |abcabcabcabcabcabcabc                                    |\n",
      "|8        |80       |abcabcabcabcabcabcabcabc                                 |\n",
      "|9        |90       |abcabcabcabcabcabcabcabcabc                              |\n",
      "|10       |100      |abcabcabcabcabcabcabcabcabcabc                           |\n",
      "|11       |110      |abcabcabcabcabcabcabcabcabcabcabc                        |\n",
      "|12       |120      |abcabcabcabcabcabcabcabcabcabcabcabc                     |\n",
      "|13       |130      |abcabcabcabcabcabcabcabcabcabcabcabcabc                  |\n",
      "|14       |140      |abcabcabcabcabcabcabcabcabcabcabcabcabcabc               |\n",
      "|15       |150      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabc            |\n",
      "|16       |160      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc         |\n",
      "|17       |170      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc      |\n",
      "|18       |180      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc   |\n",
      "|19       |190      |abcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabcabc|\n",
      "+---------+---------+---------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2dbdcd",
   "metadata": {},
   "source": [
    "Zdůrazněme, že *show* slouží opravdu jen na zobrazení prvních pár řádků. Pokud ale potřebujeme z dataframu prvních několik řádků vybrat, aby se s nimi posléze dály nějaké další operace, musí se použít metoda *limit*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72004194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------------+\n",
      "|numbers_1|numbers_2|   long_word|\n",
      "+---------+---------+------------+\n",
      "|        0|        0|            |\n",
      "|        1|       10|         abc|\n",
      "|        2|       20|      abcabc|\n",
      "|        3|       30|   abcabcabc|\n",
      "|        4|       40|abcabcabcabc|\n",
      "+---------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44057b",
   "metadata": {},
   "source": [
    "Nechat si zobrazit prvních pár řádků bývá užitečné, avšak dobré je dostat se i k nějakým informacím popisujícím celý dataframe. Již jsme si ukázali, že pokud chceme znát schéma dataframu (aka datové typy sloupců), použijeme metodu *printSchema*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b907be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numbers_1: long (nullable = true)\n",
      " |-- numbers_2: long (nullable = true)\n",
      " |-- long_word: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f389ccf",
   "metadata": {},
   "source": [
    "Shrnující statistiky přináší metoda *summary*. Jelikož vrací dataframe, musí se za její provolání nalepit *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "163909a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|summary|         numbers_1|        numbers_2|           long_word|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|  count|               100|              100|                 100|\n",
      "|   mean|              49.5|            495.0|                null|\n",
      "| stddev|29.011491975882016|290.1149197588202|                null|\n",
      "|    min|                 0|                0|                    |\n",
      "|    25%|                24|              240|                null|\n",
      "|    50%|                49|              490|                null|\n",
      "|    75%|                74|              740|                null|\n",
      "|    max|                99|              990|abcabcabcabcabcab...|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41d132d",
   "metadata": {},
   "source": [
    "Parametry *summary* jsou chtěné statistiky. Defaultně se ukazuje téměř vše, co *summary* dokáže. Výjimkou je snad jen možnost specifikovat chtěný percentil: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5e909f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+---------+\n",
      "|summary|numbers_1|numbers_2|long_word|\n",
      "+-------+---------+---------+---------+\n",
      "|    10%|        9|       90|     null|\n",
      "+-------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.summary(\"10%\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6606db",
   "metadata": {},
   "source": [
    "Extrémně podobnou funkcí je *describe*. Zde se ale jako parametry neudávají statistiky, ale sloupce, nad kterými se statistiky mají napočítat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b090a622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+\n",
      "|summary|         numbers_1|        numbers_2|           long_word|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "|  count|               100|              100|                 100|\n",
      "|   mean|              49.5|            495.0|                null|\n",
      "| stddev|29.011491975882016|290.1149197588202|                null|\n",
      "|    min|                 0|                0|                    |\n",
      "|    max|                99|              990|abcabcabcabcabcab...|\n",
      "+-------+------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1b1f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+\n",
      "|summary|        numbers_2|           long_word|\n",
      "+-------+-----------------+--------------------+\n",
      "|  count|              100|                 100|\n",
      "|   mean|            495.0|                null|\n",
      "| stddev|290.1149197588202|                null|\n",
      "|    min|                0|                    |\n",
      "|    max|              990|abcabcabcabcabcab...|\n",
      "+-------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "numbers_frame.describe(\"numbers_2\", \"long_word\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a19e8",
   "metadata": {},
   "source": [
    "Pro získání počtu řádků dataframu se v PySparku nepoužije *len*, nýbrž metoda *count*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02ccf2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers_frame.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dbf00",
   "metadata": {},
   "source": [
    "#### Manipulace se sloupci\n",
    "PySparkové dataframy jsou immutable. To znamená, že je jejich metody nemění, nýbrž po svém doběhnutí vrací nový dataframe. Ukažme si to na metodě přidávající/přepisující sloupec *withColumn*. Jejím prvním parametrem je název nového/přepisovaného sloupce, parametrem druhým pak hodnoty onoho sloupce. Pro jejich vytvoření budeme obvykle potřebovat funkce z *pyspark.sql.functions*. Kupříklad na šáhnutí na jiný sloupec se použije funkce *functions.col*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "824062cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "table_from_csv.withColumn(\"age_2\", f.col(\"age\")+10)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c9a25f",
   "metadata": {},
   "source": [
    "Vidíme že tabulka je pořád stejná. Když chceme její o sloupec bohatší verzi, musíme do proměnné vložit dataframe z výstupu metody. Připomeňme, že *withColumn* je transformace, tudíž aby se něco vůbec udělalo (nejen zobrazilo, ale aby opravdu PySpark udělal něco jiného než si jen transformaci zapsal do sady úkolů), musí se zavolat akce - v našem případě metoda *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c235d2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+-----+\n",
      "| id|first_name| last_name| age|gender|age_2|\n",
      "+---+----------+----------+----+------+-----+\n",
      "|100|    Victor|      Hugo|25.0|     M| 35.0|\n",
      "|200|      Mary|    Shelly|30.0|     F| 40.0|\n",
      "|300|    Johann|    Geothe|75.0|     M| 85.0|\n",
      "|400|    Albert|     Camus|null|     M| null|\n",
      "|500|   William|Shakespear|38.0|     M| 48.0|\n",
      "+---+----------+----------+----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = table_from_csv.withColumn(\"age_2\", f.col(\"age\")+10)\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cde48",
   "metadata": {},
   "source": [
    "Co když ale chceme vytvořit nový sloupec za použití dat starého sloupce a určité if/else logiky? Tehdy jako druhý parametr *withColumn* použijeme *functions.when*. Do této funkce vložíme jako první parametr podmínku a jako parametr druhý hodnotu, kterou má sloupec nabýt v případě její platnosti. Do ostatních buněk se vloží null. Pokud ten nechceme, pověsíme za *when* metodu *otherwise*, kde bude jako argument hodnota sloupce v případě nesplnění podmínky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b421516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+-----+---------+\n",
      "| id|first_name| last_name| age|gender|age_2|is_female|\n",
      "+---+----------+----------+----+------+-----+---------+\n",
      "|100|    Victor|      Hugo|25.0|     M| 35.0|     null|\n",
      "|200|      Mary|    Shelly|30.0|     F| 40.0|        1|\n",
      "|300|    Johann|    Geothe|75.0|     M| 85.0|     null|\n",
      "|400|    Albert|     Camus|null|     M| null|     null|\n",
      "|500|   William|Shakespear|38.0|     M| 48.0|     null|\n",
      "+---+----------+----------+----+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.withColumn(\"is_female\", f.when(f.col(\"gender\")==\"F\", 1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce8dcdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+-----+---------+\n",
      "| id|first_name| last_name| age|gender|age_2|is_female|\n",
      "+---+----------+----------+----+------+-----+---------+\n",
      "|100|    Victor|      Hugo|25.0|     M| 35.0|        0|\n",
      "|200|      Mary|    Shelly|30.0|     F| 40.0|        1|\n",
      "|300|    Johann|    Geothe|75.0|     M| 85.0|        0|\n",
      "|400|    Albert|     Camus|null|     M| null|        0|\n",
      "|500|   William|Shakespear|38.0|     M| 48.0|        0|\n",
      "+---+----------+----------+----+------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.withColumn(\"is_female\", f.when(f.col(\"gender\")==\"F\", 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e5331",
   "metadata": {},
   "source": [
    "Pro vyhození sloupce slouží metoda *drop*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec9340b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv = table_from_csv.drop(\"age_2\")\n",
    "table_from_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e96a5",
   "metadata": {},
   "source": [
    "Sloupce přejmenujeme pomocí *withColumnRenamed*, kde prvním parametrem je staré jméno sloupce a parametrem druhým jméno nové:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0683e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------------+----+------+\n",
      "| id|first_name|author_last_name| age|gender|\n",
      "+---+----------+----------------+----+------+\n",
      "|100|    Victor|            Hugo|25.0|     M|\n",
      "|200|      Mary|          Shelly|30.0|     F|\n",
      "|300|    Johann|          Geothe|75.0|     M|\n",
      "|400|    Albert|           Camus|null|     M|\n",
      "|500|   William|      Shakespear|38.0|     M|\n",
      "+---+----------+----------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.withColumnRenamed(\"last_name\", \"author_last_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29bcc20",
   "metadata": {},
   "source": [
    "Občas je potřeba provést konverzi datového typu sloupce "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0481005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: string (nullable = true)\n",
      " |-- numbers: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_types_list = [\n",
    "    [\"zero\", \"0\"],\n",
    "    [\"one\", \"1\"],\n",
    "    [\"two\", \"2\"]\n",
    "]\n",
    "data_types_frame = spark.createDataFrame(\n",
    "    data_types_list,\n",
    "    schema=[\"words\", \"numbers\"]\n",
    ")\n",
    "data_types_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f799b5b",
   "metadata": {},
   "source": [
    "To se provede s použitím metody *cast*, která obsahuje chtěný datový typ a která je vypuštěna na příslušný sloupec vybraný pomocí *functions.col*. Efektivně starý sloupec přepisujeme sloupcem novým:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd345e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- words: string (nullable = true)\n",
      " |-- numbers: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "data_types_frame = data_types_frame.withColumn(\n",
    "    \"numbers\",\n",
    "    f.col(\"numbers\").cast(IntegerType())\n",
    ")\n",
    "data_types_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a731908",
   "metadata": {},
   "source": [
    "Pokud bychom chtěli z nějakého důvodu dostat najednou data z celého dataframu, použijeme metodu *collect*. Všimněte si na níže uvedeném příkladu, že tato metoda vrací list řádků.  \n",
    "Chceme ale opravdu najednou dostat data z celého dataframu? No, obvykle ne. Znamená to totiž, že se z jednotlivých executorů pošlou data na driver, což pro dostatečně velký dataframe vyústí v \"OutOfMemory\" error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fa31d71c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id='100', first_name='Victor', last_name='Hugo', age='25.0', gender='M'),\n",
       " Row(id='200', first_name='Mary', last_name='Shelly', age='30.0', gender='F'),\n",
       " Row(id='300', first_name='Johann', last_name='Geothe', age='75.0', gender='M'),\n",
       " Row(id='400', first_name='Albert', last_name='Camus', age=None, gender='M'),\n",
       " Row(id='500', first_name='William', last_name='Shakespear', age='38.0', gender='M')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb051b3",
   "metadata": {},
   "source": [
    "Každopádně pokud už hodláme data získat pomocí collectu, můžeme z nich vyzobnout jednu buňku s pomocí indexů. Takový postup je pak vzdáleně podobný pandasímu *iloc*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cd2b1ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'William'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.collect()[4][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcfa91",
   "metadata": {},
   "source": [
    "Jak tedy provádět vybírání sloupců a řádků správně? Pro výběr sloupců se použije metoda *select*. Zdůrazněme, že narozdíl od Pandas se tu u více sloupců nutně neočekává, že budou v listu, ale mohou jít jeden za druhým jako obyčejné argumenty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "276b97fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name| last_name|\n",
      "+----------+----------+\n",
      "|    Victor|      Hugo|\n",
      "|      Mary|    Shelly|\n",
      "|    Johann|    Geothe|\n",
      "|    Albert|     Camus|\n",
      "|   William|Shakespear|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.select(\"first_name\", \"last_name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a449963",
   "metadata": {},
   "source": [
    "Nicméně *select* funguje i s listem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0525da76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name| last_name|\n",
      "+----------+----------+\n",
      "|    Victor|      Hugo|\n",
      "|      Mary|    Shelly|\n",
      "|    Johann|    Geothe|\n",
      "|    Albert|     Camus|\n",
      "|   William|Shakespear|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.select([\"first_name\", \"last_name\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd728c6",
   "metadata": {},
   "source": [
    "Pro vybrání řádků na základě hodnoty se použije metoda *filter* (resp. *where* - jedná se o alias pro *filter*). Jelikož pro potřeby takovéto operace je nutno na sloupce šáhnout, musíme opět použít metodu *col* z *pyspark.sql.functions*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "929da9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"age\")>=30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4127c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"last_name\")==\"Shakespear\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38087dbb",
   "metadata": {},
   "source": [
    "Pro znegování podmínky slouží vlnka."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0344719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+\n",
      "| id|first_name|last_name| age|gender|\n",
      "+---+----------+---------+----+------+\n",
      "|100|    Victor|     Hugo|25.0|     M|\n",
      "|200|      Mary|   Shelly|30.0|     F|\n",
      "|300|    Johann|   Geothe|75.0|     M|\n",
      "|400|    Albert|    Camus|null|     M|\n",
      "+---+----------+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(~(f.col(\"last_name\")==\"Shakespear\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c7840",
   "metadata": {},
   "source": [
    "Co se týče použití více podmínek naráz, je situace stejná jako v pandách. Tj. jednotlivé podmínky se vloží do kulatých závorek a spojí se buďto znakem & (v případě \"and\" logiky), anebo znakem | (v případě \"or\" logiky)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "409913ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(\n",
    "    (f.col(\"last_name\")==\"Shakespear\")\n",
    "    | (f.col(\"first_name\")==\"Mary\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76ab909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(\n",
    "    (f.col(\"gender\")==\"M\")\n",
    "    & (f.col(\"age\")>=30)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d66f115",
   "metadata": {},
   "source": [
    "Pro vybrání řádků s null hodnotami použijeme metodu *isNull* aplikovanou na f.col(\"jmeno_sloupce\"). Podobný modus operandi má metoda *isNotNull*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cf8f41f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+\n",
      "| id|first_name|last_name| age|gender|\n",
      "+---+----------+---------+----+------+\n",
      "|400|    Albert|    Camus|null|     M|\n",
      "+---+----------+---------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"age\").isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d4d4c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.filter(f.col(\"age\").isNotNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c774fbd5",
   "metadata": {},
   "source": [
    "Co máme dělat, když chceme selectování a filtrování provést najednou? Zkrátka selecty a filtery napíšeme za sebe. Abychom neměli jeden gigantický řádek, celý příkaz obalíme do kulatých závorek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "83a1e9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|first_name| last_name|\n",
      "+----------+----------+\n",
      "|   William|Shakespear|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "  table_from_csv\n",
    "  .filter(f.col(\"last_name\")==\"Shakespear\")\n",
    "  .select(\"first_name\", \"last_name\")\n",
    "  .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdb6ef",
   "metadata": {},
   "source": [
    "Pro seřazení řádků podle hodnot v určitém sloupci použijeme *sort*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bc79128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.sort(\"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90820768",
   "metadata": {},
   "source": [
    "Pokud chceme sestupné řazení, přidáme do *sort* parametr *ascending* s hodnotou False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cf84e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.sort(\"age\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2971bdca",
   "metadata": {},
   "source": [
    "Pokud chceme vidět unikátní hodnoty z určitého sloupce, napřed onen sloupec selectem vybereme a následně uplatníme metodu *distinct*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b750a617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|gender|\n",
      "+------+\n",
      "|     F|\n",
      "|     M|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.select(\"gender\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a24a4f",
   "metadata": {},
   "source": [
    "Pro vyhození řádků s nully se použije konstrukce *na.drop*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eb996df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33c0d11",
   "metadata": {},
   "source": [
    "Naopak pro nahrazení nullů nějakou hodotou použijeme *na.fill*. Té předáme parametr *value* říkající, čím se mají nully nahradit, a parametr *subset* specifikující, na kterých sloupcích má nahrazování probíhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5f6aefee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|   ?|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.na.fill(value=\"?\", subset=[\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84129703",
   "metadata": {},
   "source": [
    "Pro agregované statistiky napřed data shlukneme do tříd s pomocí *groupBy*. Jako parametr se uvede jméno sloupce, podle kterého grupování proběhne. Následně do metody *agg* vložíme metody (obvykle z *pyspark.sql.functions*) statistiky napočítávající. Pokud se nám nelíbí defaultní jména takto vyprodukovaných sloupců, použijeme metodu *alias*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b1184b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+--------+--------+-----------------------+\n",
      "|gender|count(1)|min(age)|max(age)|nonsensical_aggregation|\n",
      "+------+--------+--------+--------+-----------------------+\n",
      "|     F|       1|    30.0|    30.0|                  200.0|\n",
      "|     M|       4|    25.0|    75.0|                  325.0|\n",
      "+------+--------+--------+--------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_from_csv.groupBy(\"gender\").agg(\n",
    "  f.count(\"*\"),\n",
    "  f.min(\"age\"),\n",
    "  f.max(\"age\"),\n",
    "  f.avg(\"id\").alias(\"nonsensical_aggregation\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9101d",
   "metadata": {},
   "source": [
    "Pro najoinování dvou dataframů na sebe použijeme metodu *join* zavolanou na jednom z nich. Proměnná s druhým dataframem tvoří první parametr metody *join*. Druhý parametr,  *on*, specifikuje joinovací sloupec. Pokud se ten v obou tabulkách jmenuje stejně, má formu obyčejného stringu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "38e12ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+-----------+\n",
      "| id|first_name|last_name| age|gender|books_count|\n",
      "+---+----------+---------+----+------+-----------+\n",
      "|100|    Victor|     Hugo|25.0|     M|          5|\n",
      "|300|    Johann|   Geothe|75.0|     M|          7|\n",
      "|400|    Albert|    Camus|null|     M|          9|\n",
      "+---+----------+---------+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_list = [\n",
    "    [100, 5],\n",
    "    [300, 7],\n",
    "    [400, 9]\n",
    "]\n",
    "books_frame = spark.createDataFrame(\n",
    "    books_list,\n",
    "    schema=[\"id\", \"books_count\"]\n",
    ")\n",
    "\n",
    "joined_frame = table_from_csv.join(books_frame, on=\"id\")\n",
    "joined_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8387e4",
   "metadata": {},
   "source": [
    "Pokud se ale joinovací sloupec v dataframech jmenuje odlišně, obsahuje *on* porovnávání."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "301806ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+----+------+-------+-----------+\n",
      "| id|first_name|last_name| age|gender|some_id|books_count|\n",
      "+---+----------+---------+----+------+-------+-----------+\n",
      "|100|    Victor|     Hugo|25.0|     M|    100|          5|\n",
      "|300|    Johann|   Geothe|75.0|     M|    300|          7|\n",
      "|400|    Albert|    Camus|null|     M|    400|          9|\n",
      "+---+----------+---------+----+------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_list = [\n",
    "    [100, 5],\n",
    "    [300, 7],\n",
    "    [400, 9]\n",
    "]\n",
    "books_frame = spark.createDataFrame(\n",
    "    books_list,\n",
    "    schema=[\"some_id\", \"books_count\"]\n",
    ")\n",
    "\n",
    "joined_frame = table_from_csv.join(\n",
    "    books_frame, \n",
    "    on=table_from_csv[\"id\"]==books_frame[\"some_id\"]\n",
    ")\n",
    "joined_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99004a7d",
   "metadata": {},
   "source": [
    "Pakliže chceme jiný typ joinu než inner join, musíme to specifikovat do parametru *how*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bd867875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+-----------+\n",
      "| id|first_name| last_name| age|gender|books_count|\n",
      "+---+----------+----------+----+------+-----------+\n",
      "|500|   William|Shakespear|38.0|     M|       null|\n",
      "|100|    Victor|      Hugo|25.0|     M|          5|\n",
      "|200|      Mary|    Shelly|30.0|     F|       null|\n",
      "|400|    Albert|     Camus|null|     M|          9|\n",
      "|300|    Johann|    Geothe|75.0|     M|          7|\n",
      "+---+----------+----------+----+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "books_list = [\n",
    "    [100, 5],\n",
    "    [300, 7],\n",
    "    [400, 9]\n",
    "]\n",
    "books_frame = spark.createDataFrame(\n",
    "    books_list,\n",
    "    schema=[\"id\", \"books_count\"]\n",
    ")\n",
    "\n",
    "joined_frame = table_from_csv.join(books_frame, on=\"id\", how=\"left\")\n",
    "joined_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34add2",
   "metadata": {},
   "source": [
    "#### PySpark a datumy\n",
    "Dost často člověk potřebuje pracovat s datumovým datovým typem. Jelikož kvůli tomu se obvykle musí dělat konverze a i počítání rozdílů mezi dvěma dny nebývá úplně zjevné, věnujme této problematice trochu času.  \n",
    "Nejprve si vytvoříme dataframe. Všimněte si, že ačkoli druhý sloupec obsahuje de facto datumy, pro Spark jsou to stringy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69a404de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_data = [\n",
    "    [1,\"20.12.2021\"],\n",
    "    [2,\"05.01.2022\"],\n",
    "    [3,\"08.01.2022\"],\n",
    "    [4,\"12.02.2022\"]\n",
    "    \n",
    "]\n",
    "dates_frame = spark.createDataFrame(dates_data, [\"number\",\"date\"])\n",
    "dates_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c079e537",
   "metadata": {},
   "source": [
    "Konverzi provedeme s pomocí *functions.to_date*. První parametrem je konvertovaný sloupec, druhý pak obsahuje předpis popisující původní data. Význam jednotlivých písmen, které se v něm mohou objevit, nalezneme [zde](https://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eb94907a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_converted: date (nullable = true)\n",
      "\n",
      "+------+----------+--------------+\n",
      "|number|      date|date_converted|\n",
      "+------+----------+--------------+\n",
      "|     1|20.12.2021|    2021-12-20|\n",
      "|     2|05.01.2022|    2022-01-05|\n",
      "|     3|08.01.2022|    2022-01-08|\n",
      "|     4|12.02.2022|    2022-02-12|\n",
      "+------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "\n",
    "dates_frame = dates_frame.withColumn(\n",
    "    \"date_converted\",\n",
    "     f.to_date(f.col(\"date\"), \"dd.MM.yyyy\")\n",
    ")\n",
    "\n",
    "dates_frame.printSchema()\n",
    "dates_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534845ff",
   "metadata": {},
   "source": [
    "Aktuální datum získáme s pomocí *functions.current_date*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6e44cb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- number: long (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- date_converted: date (nullable = true)\n",
      " |-- current_date: date (nullable = false)\n",
      "\n",
      "+------+----------+--------------+------------+\n",
      "|number|      date|date_converted|current_date|\n",
      "+------+----------+--------------+------------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-07-18|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-07-18|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-07-18|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-07-18|\n",
      "+------+----------+--------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_frame = dates_frame.withColumn(\n",
    "    \"current_date\",\n",
    "     f.current_date()\n",
    ")\n",
    "\n",
    "dates_frame.printSchema()\n",
    "dates_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27348cfd",
   "metadata": {},
   "source": [
    "Rozdíl datumů ve dnech spočítáme prostřednictvím *functions.datediff*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "33f70e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------+------------+-----------------------+\n",
      "|number|      date|date_converted|current_date|current_minus_converted|\n",
      "+------+----------+--------------+------------+-----------------------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-07-18|                    210|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-07-18|                    194|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-07-18|                    191|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-07-18|                    156|\n",
      "+------+----------+--------------+------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates_frame.withColumn(\n",
    "    \"current_minus_converted\",\n",
    "     f.datediff(f.col(\"current_date\"), f.col(\"date_converted\"))\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a0c22",
   "metadata": {},
   "source": [
    "Pakliže potřebujeme k datu přičíst či odečíst den či měsíc, sáhneme po *functions.add_months* a *functions.date_add* (obě funkce dokáží přijmout kladné i záporné počty měsíců/dnů)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e955dd8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------+------------+------------+-------------+----------+-----------+\n",
      "|number|      date|date_converted|current_date|added_months|substr_months|added_days|substr_days|\n",
      "+------+----------+--------------+------------+------------+-------------+----------+-----------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-07-18|  2022-02-20|   2021-10-20|2022-01-04| 2021-12-05|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-07-18|  2022-03-05|   2021-11-05|2022-01-20| 2021-12-21|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-07-18|  2022-03-08|   2021-11-08|2022-01-23| 2021-12-24|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-07-18|  2022-04-12|   2021-12-12|2022-02-27| 2022-01-28|\n",
      "+------+----------+--------------+------------+------------+-------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dates_frame\n",
    "    .withColumn(\"added_months\", f.add_months(f.col(\"date_converted\"),2))\n",
    "    .withColumn(\"substr_months\", f.add_months(f.col(\"date_converted\"),-2)) \n",
    "    .withColumn(\"added_days\", f.date_add(f.col(\"date_converted\"),15)) \n",
    "    .withColumn(\"substr_days\", f.date_add(f.col(\"date_converted\"),-15))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a544ff9",
   "metadata": {},
   "source": [
    "Existují i funkce na extrakci roku, měsíce, dne atd. z datumu: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8c6cb863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------+------------+--------------+---------------+-------------+-----------+-----------+------------+\n",
      "|number|      date|date_converted|current_date|extracted_year|extracted_month|extracted_day|day_of_year|day_of_week|week_of_year|\n",
      "+------+----------+--------------+------------+--------------+---------------+-------------+-----------+-----------+------------+\n",
      "|     1|20.12.2021|    2021-12-20|  2022-07-18|          2021|             12|           20|        354|          2|          51|\n",
      "|     2|05.01.2022|    2022-01-05|  2022-07-18|          2022|              1|            5|          5|          4|           1|\n",
      "|     3|08.01.2022|    2022-01-08|  2022-07-18|          2022|              1|            8|          8|          7|           1|\n",
      "|     4|12.02.2022|    2022-02-12|  2022-07-18|          2022|              2|           12|         43|          7|           6|\n",
      "+------+----------+--------------+------------+--------------+---------------+-------------+-----------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    dates_frame\n",
    "    .withColumn(\"extracted_year\", f.year(f.col(\"date_converted\")))\n",
    "    .withColumn(\"extracted_month\", f.month(f.col(\"date_converted\"))) \n",
    "    .withColumn(\"extracted_day\", f.dayofmonth(f.col(\"date_converted\"))) \n",
    "    .withColumn(\"day_of_year\", f.dayofyear(f.col(\"date_converted\")))\n",
    "    .withColumn(\"day_of_week\", f.dayofweek(f.col(\"date_converted\")))\n",
    "    .withColumn(\"week_of_year\", f.weekofyear(f.col(\"date_converted\")))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e56ac",
   "metadata": {},
   "source": [
    "#### Window funkce\n",
    "Někdy se hodí provádět určité operace nikoli na celém dataframu, ale pouze na jeho části určené buďto hodnotou nějakého sloupce, anebo počtem řádek.  \n",
    "Nejprve si vytvořme pokusný dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "735843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_window_list = [\n",
    "    [\"one\", 10],\n",
    "    [\"one\", 10],\n",
    "    [\"one\", 30],\n",
    "    [\"one\", 40],\n",
    "    [\"one\", 50],\n",
    "    [\"two\", 60],\n",
    "    [\"two\", 70],\n",
    "    [\"two\", 80],\n",
    "    [\"two\", 90],\n",
    "    [\"three\", 100]\n",
    "    \n",
    "]\n",
    "for_window_frame = spark.createDataFrame(\n",
    "    for_window_list,\n",
    "    schema=[\"word\", \"number\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867d89fd",
   "metadata": {},
   "source": [
    "První ukázka se bude týkat číslování řádek. Nejprve si vytvoříme definici okna. Budeme chtít pro jednotlivá slova samostatné číslování, proto *partitionBy(\"word\")*. Záznamy v jedné skupině pak chceme setřídit podle hodnoty sloupce \"number\", proto následuje *orderBy(\"number\")*.   \n",
    "\n",
    "Čísla řádků budeme chtít umístit do nového sloupce, proto použijeme metodu *withColumn*. Na samotné napočítání vložíme do druhého parametru této sloupec vytvářející metody *functions.row_number().over(window_definition)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "06b6922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "| word|number|row_number|\n",
      "+-----+------+----------+\n",
      "|  two|    60|         1|\n",
      "|  two|    70|         2|\n",
      "|  two|    80|         3|\n",
      "|  two|    90|         4|\n",
      "|  one|    10|         1|\n",
      "|  one|    10|         2|\n",
      "|  one|    30|         3|\n",
      "|  one|    40|         4|\n",
      "|  one|    50|         5|\n",
      "|three|   100|         1|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"row_number\",\n",
    "    f.row_number().over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d943f35",
   "metadata": {},
   "source": [
    "Pokud bychom chtěli nikoli číslo řádky, ale pořadí čísel ve sloupci \"number\", použijeme namísto funkce *row_number* funkci *rank*. Rozdíl je patrný pro řádky, kde hodnota sloupce \"word\" je rovna \"one\" a hodnota sloupce \"number\" odpovídá 10. Zatímco u *row_number* měl u sebe jeden řádek jedničku a druhý dvojku, v případě *rank* mají jedničku oba. Následující řádek má ale trojku, tj. pořadí 2 není. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d37343eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+\n",
      "| word|number|rank|\n",
      "+-----+------+----+\n",
      "|  two|    60|   1|\n",
      "|  two|    70|   2|\n",
      "|  two|    80|   3|\n",
      "|  two|    90|   4|\n",
      "|  one|    10|   1|\n",
      "|  one|    10|   1|\n",
      "|  one|    30|   3|\n",
      "|  one|    40|   4|\n",
      "|  one|    50|   5|\n",
      "|three|   100|   1|\n",
      "+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"rank\",\n",
    "    f.rank().over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba0324",
   "metadata": {},
   "source": [
    "Pokud by nám to vadilo, použijeme namísto *rank* funkci *dense_rank*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a584c525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+\n",
      "| word|number|dense_rank|\n",
      "+-----+------+----------+\n",
      "|  two|    60|         1|\n",
      "|  two|    70|         2|\n",
      "|  two|    80|         3|\n",
      "|  two|    90|         4|\n",
      "|  one|    10|         1|\n",
      "|  one|    10|         1|\n",
      "|  one|    30|         2|\n",
      "|  one|    40|         3|\n",
      "|  one|    50|         4|\n",
      "|three|   100|         1|\n",
      "+-----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"dense_rank\",\n",
    "    f.dense_rank().over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865567c",
   "metadata": {},
   "source": [
    "Pokud potřebujeme mít sloupec, který bude o jednu či více řádek posunutý níže, aplikujeme funkci *lag*. Kde nebude co dát, tam se objeví null. Pokud bychom chtěli mít sloupec posunout opačným směrem (tj. o N řádků výše), použijeme funkci *lead*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9293613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------------+\n",
      "| word|number|lagged_column|\n",
      "+-----+------+-------------+\n",
      "|  two|    60|         null|\n",
      "|  two|    70|           60|\n",
      "|  two|    80|           70|\n",
      "|  two|    90|           80|\n",
      "|  one|    10|         null|\n",
      "|  one|    10|           10|\n",
      "|  one|    30|           10|\n",
      "|  one|    40|           30|\n",
      "|  one|    50|           40|\n",
      "|three|   100|         null|\n",
      "+-----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"lagged_column\",\n",
    "    f.lag(\"number\", 1).over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53105aab",
   "metadata": {},
   "source": [
    "Pakliže potřebujeme kumulativní sumu, aplikujeme *functions.sum*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8b4e37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------------+\n",
      "| word|number|cumulative_sum|\n",
      "+-----+------+--------------+\n",
      "|  two|    60|            60|\n",
      "|  two|    70|           130|\n",
      "|  two|    80|           210|\n",
      "|  two|    90|           300|\n",
      "|  one|    10|            20|\n",
      "|  one|    10|            20|\n",
      "|  one|    30|            50|\n",
      "|  one|    40|            90|\n",
      "|  one|    50|           140|\n",
      "|three|   100|           100|\n",
      "+-----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_definition  = Window.partitionBy(\"word\").orderBy(\"number\")\n",
    "\n",
    "for_window_frame.withColumn(\n",
    "    \"cumulative_sum\",\n",
    "    f.sum(\"number\").over(window_definition)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7b2350",
   "metadata": {},
   "source": [
    "### SQL přístup\n",
    "Doposud jsme si ukazovali práci s PySparkem, která se svým modem operandi dosti podobala práci s pandami. Nicméně existuje ještě jeden přístup, který se podobá zacházení s SQL databázemi.  \n",
    "Nejprve musíme na dataframu provolat metodu *createOrReplaceTempView*. Té předáme jen jeden parametr - jméno onoho dočasného pohledu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "231ebabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_csv.createOrReplaceTempView(\"some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12aa51c",
   "metadata": {},
   "source": [
    "Tento pohled můžeme posléze používat v sql dotazech, které vložíme jako parametr do metody *sql*, která je napojená na samotnou pysparkovou sešnu. Výsledkem je další pysparkový dataframe, na který, pokud chceme vidět jeho obsah, musíme napojit metodu *show*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1962fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+----+------+\n",
      "| id|first_name| last_name| age|gender|\n",
      "+---+----------+----------+----+------+\n",
      "|100|    Victor|      Hugo|25.0|     M|\n",
      "|200|      Mary|    Shelly|30.0|     F|\n",
      "|300|    Johann|    Geothe|75.0|     M|\n",
      "|400|    Albert|     Camus|null|     M|\n",
      "|500|   William|Shakespear|38.0|     M|\n",
      "+---+----------+----------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from some_sql_table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f667fdb1",
   "metadata": {},
   "source": [
    "### Cachování\n",
    "Při psaní této sekce jsem extenzivně čerpal  [odsud](https://towardsdatascience.com/best-practices-for-caching-in-spark-sql-b22fb0f02d34?gi=407c957f2bcb) - doporučuji přečíst.  \n",
    "Představme si, že bychom s pomocí selectování a filtrování vyrobili z nějakého dataframu dataframe menší a ten umístili do nové proměnné. Když bychom na této nové tabulce začali provádět nějaké akce, dojde bohužel k tomu, že se vytváření onoho redukovaného dataframu provede pro každou z těchto akcí zvlášť. Aby se zbytečně neplýtvalo výpočetními prostředky, existuje možnost si dataframe nacachovat, tj. uložit do paměti/na harddisk pro budoucí využití.\n",
    "Cachování se realizuje provoláním dataframové metody *cache* resp. *persist*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "12350588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, first_name: string, last_name: string, age: string, gender: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e6e95c",
   "metadata": {},
   "source": [
    "Rozdíl mezi nimi spočívá jen v tom, že u persistu lze specifikovat, kam přesně budou data uložena. Nicméně ve většině případů si člověk vystačí s defaultním MEMORY_AND_DISK, tj. data se prioritně ukládají do RAMky a když ta dojde, míří zbytek na disk.  \n",
    "V případě panda-like přístupu je cachování lazy transformace. U SQL přístupu, který má podobu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b19a479e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"cache table some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46e1ca",
   "metadata": {},
   "source": [
    "to ale neplatí - zde máme co do činění s eager operací, tj. potřebná posloupnost transformací ihned proběhne a umístí data do úložiště. Nicméně toto defaultní chování máme možnost změnit na lazy s použitím odpovídajícího klíčového slova "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96057f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"cache lazy table some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981df0f6",
   "metadata": {},
   "source": [
    "Pro zjištění, zda je dataframe nacachovaný, se použije atribut *is_cached*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d322c3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.is_cached"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d07c20",
   "metadata": {},
   "source": [
    "Pokud bychom ručně chtěli data z cache odstranit, použijeme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a278faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, first_name: string, last_name: string, age: string, gender: string]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_from_csv.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ce9f7e",
   "metadata": {},
   "source": [
    "resp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67780211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"uncache table some_sql_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9e6d70",
   "metadata": {},
   "source": [
    "Předejdeme tak situaci, kdy dojde paměť a Spark samotný začne vyhazovat tabulky na základě míry jejich nepoužívanosti. Navíc čím víc je volné paměti, tím operace probíhají rychleji (není nutné swapovat na disk). Ze stejného důvodu může být nastaven administrátorem PySparkového clusteru timeout (k nalezení v případě Cloudery v Cloudera Manageru v sekci Spark -> Configuration -> proměnná spark.dynamicAllocation.cachedExecutorIdleTimeout) - pokud se na dataframe nesáhlo po dobu delší než je tento časový úsek, je onen dataframe z cache automaticky odstraněn.  \n",
    "Stejnou motivaci má i rada cachovat nikoli celé dataframy, ale pouze ty jejich části, které potřebujeme.  \n",
    "Zmiňme nakonec, že ne vždy cachování vede k rychlejšímu běhu programu. Pokud bychom měli relativně velký parquet soubor a prováděli na něm filtrování, asi tato operace odběhne rychleji, než kdyby se musel napřed celý načíst do paměti (nebo dokonce swapovat na disk)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4152d88",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "ML algoritmy jsou vedle manipulace s daty druhou důležitou věcí, které PySpark umožňuje. Nicméně hned na začátku člověk narazí na ten problém, že tu existují dvě podobně vyhlížející knihovny - MLLib (pyspark.mllib) a ML (pyspark.ml). První z nich je starší, založená na práci s RDD a již se nevyvíjí. Proto zde budeme mluvit pouze o ML.  \n",
    "Pro začátek se podíváme na něco triviálního - vytvoříme model pro klasifikaci kosatců. Data seženeme [zde](https://archive.ics.uci.edu/ml/datasets/iris). Všimněme si, že v datovém souboru není přítomna hlavička. Musíme tak datové schéma definovat ručně, i když by PySpark asi datové typy určil správně."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74be9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "file_table_schema = StructType([\n",
    "    StructField(\"sepal_length_cm\", DoubleType()),\n",
    "    StructField(\"sepal_width_cm\", DoubleType()),\n",
    "    StructField(\"petal_length_cm\", DoubleType()),\n",
    "    StructField(\"petal_width_cm\", DoubleType()),\n",
    "    StructField(\"flower_class\", StringType())\n",
    "])\n",
    "\n",
    "\n",
    "iris_frame = (\n",
    "    spark.read.csv(\n",
    "        path=\"iris.data\",\n",
    "        header=False,\n",
    "        schema=file_table_schema\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e41383",
   "metadata": {},
   "source": [
    "Data rozdělíme na trénovací a testovací množinu pomocí metody *randomSplit*, která je navázána na pysparkovský dataframe. Abychom byli z hlediska opakovaných experimentů konzistentní, zafixujeme i *seed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc34890",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train, iris_test = iris_frame.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd1837",
   "metadata": {},
   "source": [
    "### String indexer\n",
    "PySparkovské modely se narozdíl od těch scikit-learnovských nedokáží srovnat s tím, že by byla třída záznamu uložená jako string. Pokud bychom se o to přesto pokusili, koukali bychom o pár buněk níže při samotném trénování na chybovou hlášku\n",
    "```\n",
    "(IllegalArgumentException: requirement failed: Column flower_class must be of type numeric but was actually of type string.).\n",
    "```\n",
    "Na převod třídy-stringu na třídu-číslo použijeme *StringIndexer*. Ten třídám defaultně přidělí čísla z intervalu \\[0, počet tříd). Přitom defaultně řazení bude podle míry výskytu, tj. nejzastoupenější třída bude mít index 0. Nicméně abychom věděli, které třídě odpovídá které číslo, asi bude lepší použít parametr *stringOrderType*, kterému podhodíme hodnotu \"alphabetAsc\".  \n",
    "Zdůrazněme jednu obecnou věc vlastní pysparkovským konstrukcím s *fit* a *transform*. Bez řádku, kde do dedikované proměnné (*string_indexer_iris_fit*) vkládáme výsledek *fit* metody, by nám kód nefungoval. Narozdíl od scikit-learnu totiž samotné odpálení totiž nevede k zpřístupnění transform metody.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffa8f60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+------------+--------------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|flower_class|flower_class_indexed|\n",
      "+---------------+--------------+---------------+--------------+------------+--------------------+\n",
      "|            4.3|           3.0|            1.1|           0.1| Iris-setosa|                 0.0|\n",
      "|            4.4|           2.9|            1.4|           0.2| Iris-setosa|                 0.0|\n",
      "|            4.4|           3.2|            1.3|           0.2| Iris-setosa|                 0.0|\n",
      "|            4.5|           2.3|            1.3|           0.3| Iris-setosa|                 0.0|\n",
      "|            4.6|           3.1|            1.5|           0.2| Iris-setosa|                 0.0|\n",
      "+---------------+--------------+---------------+--------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer_iris = StringIndexer(\n",
    "    inputCol=\"flower_class\", \n",
    "    outputCol=\"flower_class_indexed\", \n",
    "    stringOrderType=\"alphabetAsc\"\n",
    ")\n",
    "string_indexer_iris_fit = string_indexer_iris.fit(iris_train)\n",
    "iris_train = string_indexer_iris_fit.transform(iris_train)\n",
    "iris_test = string_indexer_iris_fit.transform(iris_test)\n",
    "\n",
    "iris_train.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432e078",
   "metadata": {},
   "source": [
    "### Vektorizace\n",
    "Nyní bychom rádi provedli škálování (scaling). U škálovacího objektu v Pysparku je věcí, kterou se předávají jména upravovaných sloupců, parametr *inputCol*. Už z jednotného čísla v názvu tohoto parametru je vidět, že list s názvy sloupců očekáván není. Neočekává se ale ani string s názvem jednoho obyčejného sloupce plného doublů či integerů. Škálované totiž mohou být pouze vektorizované sloupce umístěné v jednom výsledném sloupci. I když se ale více sloupců sloučí do jednoho, jejich data se stále naštěstí škálují separátně.  \n",
    "Co se ale myslí tou vektorizací? Hodnoty sloupců platných pro konkrétní záznam (alias jeden řádek tabulky) se umístí do vektoru (plus mínus je vektor jednorozměrné pole). Pro praktickou realizaci této operace musíme použít *VectorAssembler*, do kterého nasázíme list vstupních sloupců a jméno sloupce výstupního. Následně na takto vytvořeném objektu zavoláme metodu *transform* (*fit* tu absentuje)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84e66d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+------------+--------------------+-----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|flower_class|flower_class_indexed|vectorized_params|\n",
      "+---------------+--------------+---------------+--------------+------------+--------------------+-----------------+\n",
      "|            4.3|           3.0|            1.1|           0.1| Iris-setosa|                 0.0|[4.3,3.0,1.1,0.1]|\n",
      "|            4.4|           2.9|            1.4|           0.2| Iris-setosa|                 0.0|[4.4,2.9,1.4,0.2]|\n",
      "|            4.4|           3.2|            1.3|           0.2| Iris-setosa|                 0.0|[4.4,3.2,1.3,0.2]|\n",
      "|            4.5|           2.3|            1.3|           0.3| Iris-setosa|                 0.0|[4.5,2.3,1.3,0.3]|\n",
      "|            4.6|           3.1|            1.5|           0.2| Iris-setosa|                 0.0|[4.6,3.1,1.5,0.2]|\n",
      "+---------------+--------------+---------------+--------------+------------+--------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=[\"sepal_length_cm\", \"sepal_width_cm\", \"petal_length_cm\", \"petal_width_cm\"],\n",
    "    outputCol=\"vectorized_params\"\n",
    ")\n",
    "\n",
    "iris_train_vec = vector_assembler.transform(iris_train)\n",
    "iris_test_vec = vector_assembler.transform(iris_test)\n",
    "\n",
    "iris_train_vec.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de43853",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "Nyní už můžeme provést škálování. Škálovacích objektů existuje více (např *MinMaxScaler*), my zde použije *StandardScaler*. Pokud zapomeneme do scaleru uvést jméno výstupního sloupce, bude se jeho jméno nést v duchu \"MinMaxScaler_8c8aaa300eb8__output\". Pozor - u StandardScaleru nejsou *withMean*=True, *withStd*=True defaulty, ale bez nich výstupy moc nedávají smysl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ce6574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.896031746031745,3.041269841269841,3.8515873015873,1.2301587301587302] [0.838799217324461,0.43968531315382203,1.742170330454936,0.7524115726138021]\n",
      "+-----------------+--------------------+\n",
      "|vectorized_params|       scaled_params|\n",
      "+-----------------+--------------------+\n",
      "|[4.3,3.0,1.1,0.1]|[-1.9027577912180...|\n",
      "|[4.4,2.9,1.4,0.2]|[-1.7835397496002...|\n",
      "|[4.4,3.2,1.3,0.2]|[-1.7835397496002...|\n",
      "|[4.5,2.3,1.3,0.3]|[-1.6643217079823...|\n",
      "|[4.6,3.1,1.5,0.2]|[-1.5451036663645...|\n",
      "+-----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"vectorized_params\", outputCol=\"scaled_params\", withMean=True, withStd=True)\n",
    "scaler_fit = scaler.fit(iris_train_vec)\n",
    "iris_train_scal = scaler_fit.transform(iris_train_vec)\n",
    "iris_test_scal = scaler_fit.transform(iris_test_vec)\n",
    "\n",
    "print(scaler_fit.mean, scaler_fit.std)\n",
    "\n",
    "iris_train_scal.select(\"vectorized_params\", \"scaled_params\").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70079da5",
   "metadata": {},
   "source": [
    "### Trénování modelu\n",
    "Teď již máme data připravená k tomu, abychom je protáhli ML algortimem. Pro jednoduchost použijeme logistickou regresi. Přesněji tedy *LogisticRegression*, kterému podhodíme jednak vektorizovaný škálovaný mnohosloupec s featury (parametr *featuresCol*), jednak sloupec s štítkem třídy (parametr *labelCol*). Opět si vytvoříme separátní objekt s nafitovaným modelem, na kterém poté provoláme *transform* metodu. Ta do výsledného dataframu jednak zkopíruje svůj vstup (trénovací či testovací dataframy), jednak k němu přilepí sloupce rawProbability (hodnota, která jde do logistické funkce), probability (pravděpodobnostit toho, že záznam patří do  jednotlivých tříd) a prediction (index třídy s největší pravděpodobností)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e76217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+--------------------+----------+\n",
      "|flower_class|flower_class_indexed|       rawPrediction|         probability|prediction|\n",
      "+------------+--------------------+--------------------+--------------------+----------+\n",
      "| Iris-setosa|                 0.0|[72.9291354525068...|[1.0,4.6345127662...|       0.0|\n",
      "| Iris-setosa|                 0.0|[61.5820297479640...|[1.0,1.0695348023...|       0.0|\n",
      "| Iris-setosa|                 0.0|[74.4702770586939...|[1.0,1.8428463586...|       0.0|\n",
      "| Iris-setosa|                 0.0|[36.0972767602046...|[0.99999999999995...|       0.0|\n",
      "| Iris-setosa|                 0.0|[66.4935520663596...|[1.0,1.0682311689...|       0.0|\n",
      "+------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "log_reg_model = LogisticRegression(featuresCol=\"scaled_params\", labelCol=\"flower_class_indexed\")\n",
    "log_reg_model_fitted = log_reg_model.fit(iris_train_scal)\n",
    "\n",
    "predictions_train = log_reg_model_fitted.transform(iris_train_scal)\n",
    "predictions_test = log_reg_model_fitted.transform(iris_test_scal)\n",
    "\n",
    "predictions_train.select(\n",
    "    \"flower_class\", \"flower_class_indexed\", \"rawPrediction\",\"probability\",\"prediction\"\n",
    ").show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a402c1",
   "metadata": {},
   "source": [
    "Pakliže bychom potřebovali znát parametry modelu logistické regrese, dostaneme se k nim skrze atributy *coefficientMatrix* a *interceptVector*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31bcbb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(3, 4, [-6.3791, 16.9901, -22.5747, -21.3316, 4.1024, -7.0033, 3.658, 4.4611, 2.2767, -9.9868, 18.9167, 16.8706], 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model_fitted.coefficientMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d93e08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-5.3097, 11.1964, -5.8866])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model_fitted.interceptVector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201d85c0",
   "metadata": {},
   "source": [
    "### Vyhodnocení přesnosti modelu\n",
    "Natrénováno i oštítkováno máme, nyní bychom rádi predikce vyhodnotili. Nejprve bychom chtěli accuracy, precision, recall a f1. Jenže nemáme binární problém, třídy jsou tři. Vzpomeňme, že u scikit-learnu jsme měli pro každou metriku speciální funkci, u které jsme u vícetřídových problémů museli nastavovat parametr average. Pokud se ten nastavil jako \"macro\", spočítaly se pro každou třídu TP, FP a FN, sečetly se dohromady a na základě těchto součtů se metriky spočítaly. Oproti tomu pro average=\"micro\" se pro každou třídu spočítaly metriky separátně a tyto metriky se poté zprůměrovaly.  \n",
    "Jaký z těchto postupů byl vybrán ve Sparku? No, ani jeden - k průměrování nedochází. Člověk musí v za metriky odpovědném objektu nastavit s pomocí *metricLabel* třídu, pro kterou se spočítá metrika uvedená v parametru *metricName*. Dále se musí stanovit jméno sloupce s reálnou třídou (parametr *labelCol*) a s predikcí (parametr *predictionCol*). Následně se na takto vzniklém objektu zavolá metoda *evaluate*, které se podhodí dataframe, na kterém vyhodnocení má proběhnout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31070476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics: accuracy, class: 0, value on train: 0.9841269841269841, value on test: 1.0\n",
      "Metrics: precisionByLabel, class: 0, value on train: 1.0, value on test: 1.0\n",
      "Metrics: recallByLabel, class: 0, value on train: 1.0, value on test: 1.0\n",
      "Metrics: f1, class: 0, value on train: 0.9841269841269842, value on test: 1.0\n",
      "\n",
      "Metrics: accuracy, class: 1, value on train: 0.9841269841269841, value on test: 1.0\n",
      "Metrics: precisionByLabel, class: 1, value on train: 0.9772727272727273, value on test: 1.0\n",
      "Metrics: recallByLabel, class: 1, value on train: 0.9772727272727273, value on test: 1.0\n",
      "Metrics: f1, class: 1, value on train: 0.9841269841269842, value on test: 1.0\n",
      "\n",
      "Metrics: accuracy, class: 2, value on train: 0.9841269841269841, value on test: 1.0\n",
      "Metrics: precisionByLabel, class: 2, value on train: 0.9767441860465116, value on test: 1.0\n",
      "Metrics: recallByLabel, class: 2, value on train: 0.9767441860465116, value on test: 1.0\n",
      "Metrics: f1, class: 2, value on train: 0.9841269841269842, value on test: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for PySpark 3\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "metrics_list = [\"accuracy\", \"precisionByLabel\", \"recallByLabel\", \"f1\"]\n",
    "classes_list = [0, 1, 2]\n",
    "\n",
    "for one_class in classes_list:\n",
    "    for one_metrics in metrics_list:\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            metricName=one_metrics, \n",
    "            labelCol=\"flower_class_indexed\", \n",
    "            predictionCol=\"prediction\",\n",
    "            metricLabel=one_class\n",
    "        )\n",
    "        \n",
    "        metrics_train_value = evaluator.evaluate(predictions_train)\n",
    "        metrics_test_value = evaluator.evaluate(predictions_test)\n",
    "        \n",
    "        print(\n",
    "            f\"Metrics: {one_metrics}, class: {one_class}, \"\n",
    "            f\"value on train: {metrics_train_value}, \"\n",
    "            f\"value on test: {metrics_test_value}\"\n",
    "        )\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f676d",
   "metadata": {},
   "source": [
    "Výše uvedený kód bohužel v PySparku 2 nebude fungovat. Dvojkový Spark totiž nezná \"precisionByLabel\" a \"recallByLabel\", nýbrž pracuje s \"weightedPrecision\" a \"weightedRecall\" spočítanými z metrik pro všechny třídy. Kód by tudíž vypadal následovně:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for PySpark 2\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "metrics_list = [\"accuracy\", \"weightedPrecision\", \"weightedRecall\", \"f1\"]\n",
    "\n",
    "\n",
    "for one_metrics in metrics_list:\n",
    "    evaluator = MulticlassClassificationEvaluator(\n",
    "        metricName=one_metrics, \n",
    "        labelCol=\"flower_class_indexed\", \n",
    "        predictionCol=\"prediction\",\n",
    "        metricLabel=one_class\n",
    "    )\n",
    "        \n",
    "    metrics_train_value = evaluator.evaluate(predictions_train)\n",
    "    metrics_test_value = evaluator.evaluate(predictions_test)\n",
    "        \n",
    "    print(\n",
    "        f\"Metrics: {one_metrics}, \"\n",
    "        f\"value on train: {metrics_train_value}, \"\n",
    "        f\"value on test: {metrics_test_value}\"\n",
    "    )\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74dcb5",
   "metadata": {},
   "source": [
    "Ok, možná bychom k těmto číslům byli zpočátku nedůvěřiví. V rámci ověření bychom si chtěli vytvořit confusion matici a z ní vybrat čísla pro definici výpočtu metrik. Jenže jak onu matici vyrobit? V MLLib sice existuje *MulticlassMetrics.confusionMatrix*, ale to je jednak zastaralé, jednak určené pro RDD a ne pro dataframy. Správným řešením je metoda *crosstab* vlastní pysparkovským dataframům. Ta jak první parametr přebírá jméno sloupce, jehož hodnoty budou tvořit řádky, zatímco druhý parametr bude jméno sloupce zodpovědného za sloupce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da6499e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---+---+---+\n",
      "|flower_class_indexed_prediction|0.0|1.0|2.0|\n",
      "+-------------------------------+---+---+---+\n",
      "|                            2.0|  0|  1| 42|\n",
      "|                            1.0|  0| 43|  1|\n",
      "|                            0.0| 39|  0|  0|\n",
      "+-------------------------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_train.crosstab(\"flower_class_indexed\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc57369f",
   "metadata": {},
   "source": [
    "Pokud chceme spočítat pro multiclass model AUC (area under curve of receiver operator characteristic), tak... to nebude úplně lehké. Zejména pokud pracujeme se starší verzí PySparku. Řešení, které tu ukazuji, totiž používá funkci *vector_to_array*, která je v PySparku až od verze 3.0.0.  \n",
    "A k čemu vlastně onu funkci potřebujeme? AUC se (by definition) dá spočítat jen pro binární problém, tj. musíme mít k dispozici pravděpodobnost, že záznam patří do třídy X, a flag (0/1), že tomu tak opravdu je. Jenže v predikčních dataframech máme uložené pravděpodobnosti dohromady a navíc právě jako vektor. Z tohoto vektorového sloupce si tudíž napřed vytvoříme array sloupec, který posléze roztrhneme na tři části. Taktéž si ze sloupce s třídou vytvoříme tři flagové sloupce.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "199f2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------+------------+\n",
      "|probability_array[0]|probability_array[1]|probability_array[2]|flower_class_indexed|is_class_zero|is_class_one|is_class_two|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------+------------+\n",
      "|                 1.0|4.634512766260516...|2.069343685718057...|                 0.0|          1.0|         0.0|         0.0|\n",
      "|                 1.0|1.069534802345435...|5.452528787959966...|                 0.0|          1.0|         0.0|         0.0|\n",
      "|                 1.0|1.842846358623886...|5.109985808186763...|                 0.0|          1.0|         0.0|         0.0|\n",
      "|  0.9999999999999578|4.229003000406716...|2.204030908750582...|                 0.0|          1.0|         0.0|         0.0|\n",
      "|                 1.0|1.068231168988486...|2.177642182715996...|                 0.0|          1.0|         0.0|         0.0|\n",
      "+--------------------+--------------------+--------------------+--------------------+-------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array \n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "(\n",
    "    predictions_train\n",
    "    .withColumn(\"probability_array\", vector_to_array(\"probability\"))\n",
    "    .select([f.col(\"probability_array\")[class_index] for class_index in range(3)] + [\"flower_class_indexed\"])\n",
    "    .withColumn(\"is_class_zero\",f.when(f.col(\"flower_class_indexed\") == 0,1.0).otherwise(0.0))\n",
    "    .withColumn(\"is_class_one\",f.when(f.col(\"flower_class_indexed\") == 1,1.0).otherwise(0.0))\n",
    "    .withColumn(\"is_class_two\",f.when(f.col(\"flower_class_indexed\") == 2,1.0).otherwise(0.0))\n",
    "    .show(n=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955acf9",
   "metadata": {},
   "source": [
    "Metrika AUC je v PySpakrku dostupná skrze *BinaryClassificationEvaluator*. U toho se musí nastavit *metricName* jako \"areaUnderROC\" a *labelCol* jako sloupec s 0/1 flagem pro danou třídu, pro kterou chceme AUC spočítat. Nakonec do rawPredictionCol se musí vložit sloupec s pravděpodobností dané třídy. Člověk by si mohl myslet (i po nahlédnutí do dokumentace), že do tohoto parametru by spíš patřila rawPrediction dané třídy, ale s tím to prostě nedává očekávané výsledky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dff9fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC for class 0: 1.0\n",
      "AUC for class 1: 0.9983370288248337\n",
      "AUC for class 2: 0.9983188568226394\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "for_auc_train_frame = (\n",
    "    predictions_train\n",
    "    .withColumn(\"probability_array\", vector_to_array(\"probability\"))\n",
    "    .select([f.col(\"probability_array\")[class_index] for class_index in range(3)] + [\"flower_class_indexed\"])\n",
    "    .withColumn(\"is_class_zero\",f.when(f.col(\"flower_class_indexed\") == 0,1.0).otherwise(0.0))\n",
    "    .withColumn(\"is_class_one\",f.when(f.col(\"flower_class_indexed\") == 1,1.0).otherwise(0.0))\n",
    "    .withColumn(\"is_class_two\",f.when(f.col(\"flower_class_indexed\") == 2,1.0).otherwise(0.0))\n",
    "    )\n",
    "\n",
    "col_names = [\n",
    "    (\"probability_array[0]\", \"is_class_zero\"),\n",
    "    (\"probability_array[1]\", \"is_class_one\"),\n",
    "    (\"probability_array[2]\", \"is_class_two\")\n",
    "]\n",
    "\n",
    "for class_index, one_name_tuple in enumerate(col_names):\n",
    "    \n",
    "    binary_evaluator = BinaryClassificationEvaluator(\n",
    "        metricName=\"areaUnderROC\",\n",
    "        rawPredictionCol=one_name_tuple[0],\n",
    "        labelCol=one_name_tuple[1]\n",
    "    )\n",
    "    \n",
    "    metrics_train_value = binary_evaluator.evaluate(for_auc_train_frame)\n",
    "    \n",
    "    print(f\"AUC for class {class_index}: {metrics_train_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221f8f9",
   "metadata": {},
   "source": [
    "### Feature selection\n",
    "Představme si, že bychom měli více než 4 feature sloupce a potřebovali jejich počet redukovat. Na to lze použít [chi-squared test] (https://en.wikipedia.org/wiki/Chi-squared_test) implementovaný v podobě třídy *ChiSqSelector*. Do jejího konstruktoru vložíme parametr *numTopFeaturesnumTopFeatures* říkající, kolik nejdůležitějších featurů chceme na výstupu mít. Alternativně by šlo uvést jejich procento v parametru *percentile* či by šlo použít pár dalších podmínek. Do parametru  *featuresCol* se vloží jméno vektorizovaného sloupce s featurami, *outputCol* bude obsahovat jméno nového sloupce obsahujícího jen vybrané featury a *labelCol* jméno sloupce s indexy tříd. Následuje obvyklé kolečko s fitem a transformací framu.  \n",
    "Popravdě mi není úplně jasné, proč následující kód nevyvolá chybu. Přeci jen naše featury obsahují reálná čísla a nikoli diskrétní kategorie, které by chi-square z definice vyžadovalo. Možná se v implementaci provádí bucketování? Anebo zkrátka v definici třídy žádná kontrola není a dostáváme úplné nesmysly :D, což je asi nejpravděpodobnější varianta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "601561da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----------------------------------------+\n",
      "|scaled_params                                                                     |selected_scaled_params                   |\n",
      "+----------------------------------------------------------------------------------+-----------------------------------------+\n",
      "|[-1.9027577912180789,-0.09386222381142634,-1.5794019984651975,-1.5020485746021588]|[-1.5794019984651975,-1.5020485746021588]|\n",
      "|[-1.7835397496002372,-0.3212976122775758,-1.4072029919985565,-1.3691425911752826] |[-1.4072029919985565,-1.3691425911752826]|\n",
      "+----------------------------------------------------------------------------------+-----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "\n",
    "selector = ChiSqSelector(\n",
    "    numTopFeatures=2, \n",
    "    featuresCol=\"scaled_params\", \n",
    "    outputCol=\"selected_scaled_params\",\n",
    "    labelCol=\"flower_class_indexed\"\n",
    ")\n",
    "\n",
    "fitted_selector = selector.fit(iris_train_scal)\n",
    "selected_frame = fitted_selector.transform(iris_train_scal)\n",
    "\n",
    "selected_frame.select(\"scaled_params\",\"selected_scaled_params\").show(n=2, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35c17b8",
   "metadata": {},
   "source": [
    "Pokud chceme vědět, které sloupce výběrem prošly, podíváme se u nafitovaného selectoru na atribut *selectedFeatures*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fd8e912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_selector.selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765c255c",
   "metadata": {},
   "source": [
    "Podle dokumentace je nicméně od verze 3.1.0 *ChiSqSelector* ve stavu deprecated s tím, že by se měl namísto něho používat *UnivariateFeatureSelector* (v PySparku od verze 3.1.1). V jeho konstruktoru musíme v parametru *selectionMode* specifikovat,  jakých způsobem chceme stanovit množství sloupců, které selectorem prolezou - v příkladu volíme \"numTopFeatures\". Avšak počet sloupců se nenastavuje v konstuktoru, ale až posléze v metodě *setSelectionThreshold*. Na stejném místě se s pomocí metod *setFeatureType* a *setLabelType* PySparku říká, zda jsou featury a labely kontinuální či kategorické. Na základě toho se PySpark pokusí o redukci sloupců pomocí chi-square (featury i labely kategorické), ANOVY (featury spojité, labely kategorické) či jakési F-value (featury i labely spojité)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "439e6215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+-----------------------------------------+\n",
      "|scaled_params                                                                     |selected_scaled_params                   |\n",
      "+----------------------------------------------------------------------------------+-----------------------------------------+\n",
      "|[-1.9027577912180789,-0.09386222381142634,-1.5794019984651975,-1.5020485746021588]|[-1.9027577912180789,-1.5794019984651975]|\n",
      "|[-1.7835397496002372,-0.3212976122775758,-1.4072029919985565,-1.3691425911752826] |[-1.7835397496002372,-1.4072029919985565]|\n",
      "+----------------------------------------------------------------------------------+-----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only PySpark 3\n",
    "from pyspark.ml.feature import UnivariateFeatureSelector\n",
    "\n",
    "selector = UnivariateFeatureSelector(\n",
    "    selectionMode=\"numTopFeatures\", \n",
    "    featuresCol=\"scaled_params\", \n",
    "    outputCol=\"selected_scaled_params\",\n",
    "    labelCol=\"flower_class_indexed\"\n",
    ")\n",
    "selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(2)\n",
    "fitted_selector = selector.fit(iris_train_scal)\n",
    "selected_frame = fitted_selector.transform(iris_train_scal)\n",
    "\n",
    "selected_frame.select(\"scaled_params\",\"selected_scaled_params\").show(n=2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f56a4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_selector.selectedFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b9fd54",
   "metadata": {},
   "source": [
    "### Gridsearch\n",
    "I relativně jednoduché modely mají hromadu parametrů. Jejich vysvětlení lze nahlédnout s použitím metody *explainParams*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d95a06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\\nfamily: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\\nfeaturesCol: features column name. (default: features, current: scaled_params)\\nfitIntercept: whether to fit an intercept term. (default: True)\\nlabelCol: label column name. (default: label, current: flower_class_indexed)\\nlowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\\nlowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\\nmaxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\\nmaxIter: max number of iterations (>= 0). (default: 100)\\npredictionCol: prediction column name. (default: prediction)\\nprobabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\\nrawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\\nregParam: regularization parameter (>= 0). (default: 0.0)\\nstandardization: whether to standardize the training features before fitting the model. (default: True)\\nthreshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\\nthresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\\ntol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\\nupperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\\nupperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model_fitted.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e028a66",
   "metadata": {},
   "source": [
    "Faktické hodnoty těchto parametrů obdržíme po provolání metody *extractParamMap*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea6824c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_01b9ff7ff564', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='featuresCol', doc='features column name.'): 'scaled_params',\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='labelCol', doc='label column name.'): 'flower_class_indexed',\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_01b9ff7ff564', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model_fitted.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f223fe8e",
   "metadata": {},
   "source": [
    "Pokud chceme znát hodnotu jen jednoho parametru, můžeme taktéž použít provolání typu jmeno_modelu.\\_java_obj.parent().getXYZ, kde XYZ je název parametru (první písmeno slova za getem je velké!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9f09832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg_model_fitted._java_obj.parent().getRegParam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae73da",
   "metadata": {},
   "source": [
    "V případě, kdy chceme vyzkoušet, jak bude model performovat pro větší počet hodnot různých parametrů, musíme sáhnout po crossvalidaci. Nejprve si vyrobíme instanci chtěného modelu. Následně vytvoříme mapu parametrů s pomocí *ParamGridBuilder*. Parametry k testování do něj vkládáme pomocí metody *addGrid* - u ní je prvním parametrem kýžený parametr modelu, druhým parametrem pak list se seznamem potenciálních hodnot parametru (to je tedy věta). Na konci tohoto řetězu musí být metoda *build*. Posléze si vytvoříme evaluátor, podle kterého budeme jednotlivé model s různými parametry poměřovat. Nakonec vyrobíme cross validátor, do kterého nasypeme instanci modelu (parametr *estimator*), seznam parametrů a jejich hodnot (*estimatorParamMaps*), evaluátor (*evaluator*) a počet foldů, na kterých bude cross validace probíhat (*numFolds*). Nyní se k cross validátoru chováme jako k obyčejnému modelu, tj. následuje klasické kolečko fitu a transformace, přičemž na konci je predikční dataframe vyrobený modelem s nejlepšími parametry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30f6c5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+------------+--------------------+-----------------+--------------------+--------------------+--------------------+----------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|flower_class|flower_class_indexed|vectorized_params|       scaled_params|       rawPrediction|         probability|prediction|\n",
      "+---------------+--------------+---------------+--------------+------------+--------------------+-----------------+--------------------+--------------------+--------------------+----------+\n",
      "|            4.3|           3.0|            1.1|           0.1| Iris-setosa|                 0.0|[4.3,3.0,1.1,0.1]|[-1.9027577912180...|[0.23203651806669...|[0.41241663443775...|       0.0|\n",
      "|            4.4|           2.9|            1.4|           0.2| Iris-setosa|                 0.0|[4.4,2.9,1.4,0.2]|[-1.7835397496002...|[0.19368770028198...|[0.39875039812181...|       0.0|\n",
      "+---------------+--------------+---------------+--------------+------------+--------------------+-----------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PySpark 3\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "log_reg_model = LogisticRegression(featuresCol=\"scaled_params\", labelCol=\"flower_class_indexed\")\n",
    "\n",
    "parameter_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(log_reg_model.elasticNetParam, [0, 1])\n",
    "    .addGrid(log_reg_model.regParam, [5, 10, 100])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    metricName=\"f1\", \n",
    "    labelCol=\"flower_class_indexed\", \n",
    "    predictionCol=\"prediction\",\n",
    "    metricLabel=0\n",
    ")\n",
    "\n",
    "crossvalidator = CrossValidator(\n",
    "    estimator=log_reg_model,\n",
    "    estimatorParamMaps=parameter_grid,\n",
    "    evaluator=multi_evaluator,\n",
    "    numFolds=5    \n",
    ")\n",
    "\n",
    "crossval_model = crossvalidator.fit(iris_train_scal)\n",
    "predictions_train = crossval_model.transform(iris_train_scal)\n",
    "\n",
    "predictions_train.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47fc49",
   "metadata": {},
   "source": [
    "V dvojkovém PySparku nemůžeme mít v *MulticlassClassificationEvaluator* parametr *metricLabel*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2686c3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PySpark 2\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "log_reg_model = LogisticRegression(featuresCol=\"scaled_params\", labelCol=\"flower_class_indexed\")\n",
    "\n",
    "parameter_grid = (\n",
    "    ParamGridBuilder()\n",
    "    .addGrid(log_reg_model.elasticNetParam, [0, 1])\n",
    "    .addGrid(log_reg_model.regParam, [5, 10, 100])\n",
    "    .build()\n",
    ")\n",
    "\n",
    "multi_evaluator = MulticlassClassificationEvaluator(\n",
    "    metricName=\"f1\", \n",
    "    labelCol=\"flower_class_indexed\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "crossvalidator = CrossValidator(\n",
    "    estimator=log_reg_model,\n",
    "    estimatorParamMaps=parameter_grid,\n",
    "    evaluator=multi_evaluator,\n",
    "    numFolds=5    \n",
    ")\n",
    "\n",
    "crossval_model = crossvalidator.fit(iris_train_scal)\n",
    "predictions_train = crossval_model.transform(iris_train_scal)\n",
    "\n",
    "predictions_train.show(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b955945",
   "metadata": {},
   "source": [
    "Nejlepší model můžeme explicitně vybrat z atributu nafitovaného cross validátoru *bestModel*. Zvolené hodnoty atributů si opět zobrazíme pomocí *extractParamMap*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d161ebc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LogisticRegression_62751db598fc', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto',\n",
       " Param(parent='LogisticRegression_62751db598fc', name='featuresCol', doc='features column name.'): 'scaled_params',\n",
       " Param(parent='LogisticRegression_62751db598fc', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='labelCol', doc='label column name.'): 'flower_class_indexed',\n",
       " Param(parent='LogisticRegression_62751db598fc', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LogisticRegression_62751db598fc', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability',\n",
       " Param(parent='LogisticRegression_62751db598fc', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LogisticRegression_62751db598fc', name='regParam', doc='regularization parameter (>= 0).'): 5.0,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5,\n",
       " Param(parent='LogisticRegression_62751db598fc', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = crossval_model.bestModel\n",
    "best_model.extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90685cb",
   "metadata": {},
   "source": [
    "### Pipelina\n",
    "V praxi nebudeme chtít mít model a nutné data zpracovávací operace rozstrkané v jednotlivých objektech. Namísto toho si budeme přát mít všechno uložené v jednom objektu - v pipelině. Objekty zpracovávající sloupce (ať už featury, anebo labely) si vyrobíme jako obvykle. Pouze v případech, kdy jeden objekt-operace bude používat sloupce vytvořené operací předešlou, nenapíšeme do parametru *inputCol* jméno sloupce jako string, ale *nazev_predesleho_objektu_v_pipeline.getOutputCol()*. Nakonec si vytvoříme objekt typu *Pipeline*, do jejíhož konstruktoru vložíme do parametru *stages* list s operacemi/objekty, jak by měly jít za sebou. S pipelinou pak můžeme zacházet jako s obyčejným modelem, tj. lze použít *fit* a *transform* operace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63d5b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+----------+\n",
      "|   flower_class|flower_class_indexed|prediction|\n",
      "+---------------+--------------------+----------+\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|Iris-versicolor|                 1.0|       1.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|Iris-versicolor|                 1.0|       1.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|    Iris-setosa|                 0.0|       0.0|\n",
      "|Iris-versicolor|                 1.0|       1.0|\n",
      "|Iris-versicolor|                 1.0|       1.0|\n",
      "|Iris-versicolor|                 1.0|       1.0|\n",
      "| Iris-virginica|                 2.0|       2.0|\n",
      "|Iris-versicolor|                 1.0|       1.0|\n",
      "| Iris-virginica|                 2.0|       2.0|\n",
      "| Iris-virginica|                 2.0|       2.0|\n",
      "+---------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "iris_train, iris_test = iris_frame.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "cols_for_scaling = [\"sepal_length_cm\", \"sepal_width_cm\", \"petal_length_cm\", \"petal_width_cm\"]\n",
    "\n",
    "vector_col_to_scale = VectorAssembler(\n",
    "    inputCols=cols_for_scaling, \n",
    "    outputCol=\"vectorized_cols\"\n",
    ")\n",
    "scaling_stage = StandardScaler(\n",
    "    inputCol=vector_col_to_scale.getOutputCol(), \n",
    "    outputCol=\"scaledFeatures\"\n",
    ")\n",
    "\n",
    "target_indexer = StringIndexer(\n",
    "    inputCol=\"flower_class\", \n",
    "    outputCol=\"flower_class_indexed\", \n",
    "    stringOrderType=\"alphabetAsc\"\n",
    ")\n",
    "\n",
    "ml_model = LogisticRegression(\n",
    "    featuresCol=scaling_stage.getOutputCol(), \n",
    "    labelCol=target_indexer.getOutputCol()\n",
    ")\n",
    "\n",
    "classification_pipeline = Pipeline(\n",
    "    stages=[vector_col_to_scale, scaling_stage, target_indexer, ml_model]\n",
    ")\n",
    "\n",
    "fited_model = classification_pipeline.fit(iris_train)\n",
    "\n",
    "prediction_train = fited_model.transform(iris_train)\n",
    "prediction_test = fited_model.transform(iris_test)\n",
    "\n",
    "prediction_test.select(\"flower_class\", \"flower_class_indexed\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873919e9",
   "metadata": {},
   "source": [
    "### Ukládání modelu\n",
    "Model uložíme pomocí metody *save*. Na lokále při naší windows instalaci tento příkaz povede k pádu. Na hdfs se ale model uloží - defaultně do user složky. Zdůrazněme, že narozdíl od klasických jednosouborových picklů má v případě PySparku uložený model podobu vcelku složité adresářové struktury."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193bb4ed",
   "metadata": {},
   "source": [
    "```python\n",
    "fited_model.save(\"saved_model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e522b30",
   "metadata": {},
   "source": [
    "Pro načtení modelu musíme napřed naimportovat odpovídající typ modelu. Nicméně ne ten samý, který jsme importovali při trénovaní. Namísto toho musíme chtít JmenoTypuModelu**Model** (tj. například u lineární regrese musíme z  *pyspark.ml.classification* naimportovat *LogisticRegressionModel*). V případě uložené pipeliny potřebujeme *PipelineModel*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574f54d",
   "metadata": {},
   "source": [
    "```python\n",
    "from  pyspark.ml import PipelineModel\n",
    "loaded_model = PipelineModel.load(\"saved_model\")\n",
    "\n",
    "prediction_test = loaded_model.transform(iris_test)\n",
    "prediction_test.select(\"flower_class\", \"flower_class_indexed\", \"prediction\").show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61ab1",
   "metadata": {},
   "source": [
    "### Oversampling v PySparku\n",
    "V aktuálních kosatcových datech jsou všechny třídy zastoupeny stejnoměrně. Co kdyby ale byl počet záznamů patřících k jedné třídě řádově menší než počty záznamů z tříd ostatních? Tehdy by nastala potřeba provést oversampling, který v té nejjednodušší formě nabývá podoby naklonování minoritních dat. Bohužel narozdíl od obyčejného Pythonu při práci s PySparkem přístup k balíčku imbalanced-learn nemáme. Nezbývá tudíž nic jiného oversamplování provést s pomocí metody *sample*.  \n",
    "Napřed si vytvořme dataframe, ve kterém bude pouze minoritní třída - třídy ostatní budou v separátním datasetu. Na tomto dataframu provoláme *sample*. Zde si musíme dát pozor, abychom jednak nezapomenuli na parametr *withReplacement*, jednak aby v parametru *fraction* říkajícím, kolikrát má být výsledný dataframe větší než ten původní, byl float. Tj. i kdybychom chtěli počet záznamů minoritní třídy nafouknout třikrát, nepíšeme zde 3, nýbrž 3.0.  \n",
    "Nakonec musíme spojit oversamplovaný frame s framem pro zbylé třídy. To provedeme provoláním metody *union* na jednom z nich s tím, že dataframe druhý je parametrem unionu.  \n",
    "Všimněme si, že metoda *sample* není co do počtu vyprodukovaných záznamů nikteram přesná. Tato cifra se bude lišit spuštění od spuštění, což říká ostatně i samotná [dokumentace](https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.DataFrame.sample.html).  \n",
    "\n",
    "Pozn.: tento příklad je přitažený za vlasy, neboť fakticky v něm imbalanced dataframe vyrábíme. Nicméně pokud bych chtěl napřed část záznamů jedné ze tříd odstranit, musel bych opět použít sample, což by vysvětlení spíše zatemnilo než osvětlilo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "787b43b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+\n",
      "|   flower_class|count(1)|\n",
      "+---------------+--------+\n",
      "| Iris-virginica|      50|\n",
      "|    Iris-setosa|     197|\n",
      "|Iris-versicolor|      50|\n",
      "+---------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "dataframe_only_class_setosa = iris_frame.filter(f.col(\"flower_class\")==\"Iris-setosa\")\n",
    "dataframe_other_classes = iris_frame.filter(f.col(\"flower_class\")!=\"Iris-setosa\")\n",
    "\n",
    "dataframe_setosa_oversampled = dataframe_only_class_setosa.sample(withReplacement=True, fraction=4.0)\n",
    "oversampled_frame = dataframe_other_classes.union(dataframe_setosa_oversampled)\n",
    "\n",
    "oversampled_frame.groupby(\"flower_class\").agg(f.count(\"*\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5bfeb7",
   "metadata": {},
   "source": [
    "### Bucketování\n",
    "Zejména při práci s datumy bývá užitečné hodnoty určitého sloupce roztřídit do předem připravených kategorií. Takovouto operaci obstarává v PySparku *Buckerizer*. Do jeho konstruktoru se krom vstupního a výstupního sloupce musí do parametru *split* uvést, jaké jsou hranice jednotlivých intervalů. Je třeba, aby tato čísla zahrnovala celou množinu hodnot ve vstupních sloupci - v opačném případě se setkáme s hláškou\n",
    "```\n",
    "IllegalArgumentException: Bucketizer_e751d7c9ec1e parameter splits given invalid value [5.0,6.0].\n",
    "```\n",
    "Následně už lze vytvořit nový dataframe s použitím *transform* metody bucketizerovského objektu (fit není třeba)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea0b518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+------------+------------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|flower_class|bucketized_sep_len|\n",
      "+---------------+--------------+---------------+--------------+------------+------------------+\n",
      "|            5.1|           3.5|            1.4|           0.2| Iris-setosa|               1.0|\n",
      "|            4.9|           3.0|            1.4|           0.2| Iris-setosa|               0.0|\n",
      "|            4.7|           3.2|            1.3|           0.2| Iris-setosa|               0.0|\n",
      "|            4.6|           3.1|            1.5|           0.2| Iris-setosa|               0.0|\n",
      "|            5.0|           3.6|            1.4|           0.2| Iris-setosa|               1.0|\n",
      "|            5.4|           3.9|            1.7|           0.4| Iris-setosa|               1.0|\n",
      "|            4.6|           3.4|            1.4|           0.3| Iris-setosa|               0.0|\n",
      "|            5.0|           3.4|            1.5|           0.2| Iris-setosa|               1.0|\n",
      "|            4.4|           2.9|            1.4|           0.2| Iris-setosa|               0.0|\n",
      "|            4.9|           3.1|            1.5|           0.1| Iris-setosa|               0.0|\n",
      "|            5.4|           3.7|            1.5|           0.2| Iris-setosa|               1.0|\n",
      "|            4.8|           3.4|            1.6|           0.2| Iris-setosa|               0.0|\n",
      "|            4.8|           3.0|            1.4|           0.1| Iris-setosa|               0.0|\n",
      "|            4.3|           3.0|            1.1|           0.1| Iris-setosa|               0.0|\n",
      "|            5.8|           4.0|            1.2|           0.2| Iris-setosa|               2.0|\n",
      "|            5.7|           4.4|            1.5|           0.4| Iris-setosa|               2.0|\n",
      "|            5.4|           3.9|            1.3|           0.4| Iris-setosa|               1.0|\n",
      "|            5.1|           3.5|            1.4|           0.3| Iris-setosa|               1.0|\n",
      "|            5.7|           3.8|            1.7|           0.3| Iris-setosa|               2.0|\n",
      "|            5.1|           3.8|            1.5|           0.3| Iris-setosa|               1.0|\n",
      "+---------------+--------------+---------------+--------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketizer_sepal_length = Bucketizer(\n",
    "    splits=[0, 5.0, 5.5, 6.0, 10], \n",
    "    inputCol=\"sepal_length_cm\", outputCol=\"bucketized_sep_len\"\n",
    ")\n",
    "bucketized_frame = bucketizer_sepal_length.transform(iris_frame)\n",
    "bucketized_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0158294",
   "metadata": {},
   "source": [
    "### Imputing\n",
    "Ukažme si ještě jeden příklad vytvoření ML modelu ve Sparku. Tentokrát se bude jednat o regresi - budeme se snažit určit hodnocení cereálií na základě jejich výživových i jiných vlastností. Data jsou k nalezení [zde](https://perso.telecom-paristech.fr/eagan/class/igr204/datasets), jejich popis je [zde](http://lib.stat.cmu.edu/datasets/1993.expo/).  \n",
    "\n",
    "Nejprve si data načtěme. Všimněme si, že na druhém řádku jsou datové typy, nikoli relevantní záznam. Kdybychom tuto skutečnost neošetřili, buďto by se nám onen špatný záznam vložil na první řádek tabulky (PySpark 3), anebo by kvůli konfliktu se schématem byla tabulka nablněná samými nully (PySpark 2). Řešení spočívá v přidání parametru *mode* s hodnotou *dropmalformed*.  \n",
    "\n",
    "Pakliže pracujeme s PySparkem 2, bude nutné nastavit datové typy pro sloupce \"sugars\" a  \"potass\" na DoubleType. Pokud bychom ponechali IntegerType, selhalo by imputování."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e202a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    -1|      25|    3|   1.0|0.75|34.384843|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "file_table_schema = StructType([\n",
    "    StructField(\"name\", StringType()),\n",
    "    StructField(\"mfr\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"calories\", IntegerType()),\n",
    "    StructField(\"protein\", IntegerType()),\n",
    "    StructField(\"fat\", IntegerType()),\n",
    "    StructField(\"sodium\", IntegerType()),\n",
    "    StructField(\"fiber\", DoubleType()),\n",
    "    StructField(\"carbo\", DoubleType()),\n",
    "    StructField(\"sugars\", IntegerType()),\n",
    "    StructField(\"potass\", IntegerType()),\n",
    "    StructField(\"vitamins\", IntegerType()),\n",
    "    StructField(\"shelf\", IntegerType()),\n",
    "    StructField(\"weight\", DoubleType()),\n",
    "    StructField(\"cups\", DoubleType()),\n",
    "    StructField(\"rating\", DoubleType())\n",
    "])\n",
    "\n",
    "\n",
    "cereal_frame = (\n",
    "    spark.read.csv(\n",
    "        path=\"cereal.csv\",\n",
    "        header=True,\n",
    "        sep=\";\",\n",
    "        schema=file_table_schema,\n",
    "        mode=\"dropmalformed\"\n",
    "    )\n",
    ")\n",
    "\n",
    "cereal_frame.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf1f519",
   "metadata": {},
   "source": [
    "Vidíme, že ve sloupci potass (aka potassium neboli draslík) je místo váhy v miligramech mínus jednička. Nevyskytuje se něco podobného i v dalších sloupcích? Vypusťme na dataframe metodu *summary*. Jelikož PySparkový výstup moc čitelný není a tabulka produkovaná *summary* metodou je opravdu malá, převedeme ji do pandího dataframu.  \n",
    "Vidíme, že mínus jednička se kromě sloupce *potass* objevuje i v *carbo* a *sugars*. Nedá se předpokládat, že by v nějakých cereáliích bylo nula sacharidů. Spíš byla hodnota neznámá. Abychom s odpovídajícími záznamy mohli dále pracovat, musíme na místo mínus jedničky vložit nějakou rozumnou hodnotu, třeba průměr zbylých hodnot v dotčeném sloupci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7ddfc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>name</th>\n",
       "      <th>mfr</th>\n",
       "      <th>type</th>\n",
       "      <th>calories</th>\n",
       "      <th>protein</th>\n",
       "      <th>fat</th>\n",
       "      <th>sodium</th>\n",
       "      <th>fiber</th>\n",
       "      <th>carbo</th>\n",
       "      <th>sugars</th>\n",
       "      <th>potass</th>\n",
       "      <th>vitamins</th>\n",
       "      <th>shelf</th>\n",
       "      <th>weight</th>\n",
       "      <th>cups</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>106.88311688311688</td>\n",
       "      <td>2.5454545454545454</td>\n",
       "      <td>1.0129870129870129</td>\n",
       "      <td>159.67532467532467</td>\n",
       "      <td>2.1519480519480516</td>\n",
       "      <td>14.597402597402597</td>\n",
       "      <td>6.922077922077922</td>\n",
       "      <td>96.07792207792208</td>\n",
       "      <td>28.246753246753247</td>\n",
       "      <td>2.207792207792208</td>\n",
       "      <td>1.0296103896103894</td>\n",
       "      <td>0.8210389610389613</td>\n",
       "      <td>42.66570498701299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>19.484119056820845</td>\n",
       "      <td>1.0947897484455342</td>\n",
       "      <td>1.0064725594803927</td>\n",
       "      <td>83.83229524009316</td>\n",
       "      <td>2.3833639643872226</td>\n",
       "      <td>4.278956280325907</td>\n",
       "      <td>4.4448853924193585</td>\n",
       "      <td>71.2868125092621</td>\n",
       "      <td>22.342522500566307</td>\n",
       "      <td>0.832524100135788</td>\n",
       "      <td>0.15047679973689213</td>\n",
       "      <td>0.23271613844691388</td>\n",
       "      <td>14.047288743735216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>100% Bran</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>18.042851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25%</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.67</td>\n",
       "      <td>33.174094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50%</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>40.400208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>75%</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>210</td>\n",
       "      <td>3.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>11</td>\n",
       "      <td>120</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.828392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max</td>\n",
       "      <td>Wheaties Honey Gold</td>\n",
       "      <td>R</td>\n",
       "      <td>H</td>\n",
       "      <td>160</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>320</td>\n",
       "      <td>14.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>15</td>\n",
       "      <td>330</td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>93.704912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary                 name   mfr  type            calories  \\\n",
       "0   count                   77    77    77                  77   \n",
       "1    mean                 None  None  None  106.88311688311688   \n",
       "2  stddev                 None  None  None  19.484119056820845   \n",
       "3     min            100% Bran     A     C                  50   \n",
       "4     25%                 None  None  None                 100   \n",
       "5     50%                 None  None  None                 110   \n",
       "6     75%                 None  None  None                 110   \n",
       "7     max  Wheaties Honey Gold     R     H                 160   \n",
       "\n",
       "              protein                 fat              sodium  \\\n",
       "0                  77                  77                  77   \n",
       "1  2.5454545454545454  1.0129870129870129  159.67532467532467   \n",
       "2  1.0947897484455342  1.0064725594803927   83.83229524009316   \n",
       "3                   1                   0                   0   \n",
       "4                   2                   0                 130   \n",
       "5                   3                   1                 180   \n",
       "6                   3                   2                 210   \n",
       "7                   6                   5                 320   \n",
       "\n",
       "                fiber               carbo              sugars  \\\n",
       "0                  77                  77                  77   \n",
       "1  2.1519480519480516  14.597402597402597   6.922077922077922   \n",
       "2  2.3833639643872226   4.278956280325907  4.4448853924193585   \n",
       "3                 0.0                -1.0                  -1   \n",
       "4                 1.0                12.0                   3   \n",
       "5                 2.0                14.0                   7   \n",
       "6                 3.0                17.0                  11   \n",
       "7                14.0                23.0                  15   \n",
       "\n",
       "              potass            vitamins              shelf  \\\n",
       "0                 77                  77                 77   \n",
       "1  96.07792207792208  28.246753246753247  2.207792207792208   \n",
       "2   71.2868125092621  22.342522500566307  0.832524100135788   \n",
       "3                 -1                   0                  1   \n",
       "4                 40                  25                  1   \n",
       "5                 90                  25                  2   \n",
       "6                120                  25                  3   \n",
       "7                330                 100                  3   \n",
       "\n",
       "                weight                 cups              rating  \n",
       "0                   77                   77                  77  \n",
       "1   1.0296103896103894   0.8210389610389613   42.66570498701299  \n",
       "2  0.15047679973689213  0.23271613844691388  14.047288743735216  \n",
       "3                  0.5                 0.25           18.042851  \n",
       "4                  1.0                 0.67           33.174094  \n",
       "5                  1.0                 0.75           40.400208  \n",
       "6                  1.0                  1.0           50.828392  \n",
       "7                  1.5                  1.5           93.704912  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cereal_frame.summary().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514319a9",
   "metadata": {},
   "source": [
    "BTW pokud bychom chtěli vidět minimum jen u jednoho sloupce, použijeme následující kód:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86a47b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|min(carbo)|\n",
      "+----------+\n",
      "|      -1.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "cereal_frame.select(f.min(\"carbo\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c77fc94",
   "metadata": {},
   "source": [
    "Nahrazování provedeme s pomocí *Imputeru*. Ten napřed z imputovaného sloupce odfiltruje hodnoty určené v parametru *missingValue* (jeho defaultní hodnota je nan) a na hodnotách zbylých provede výpočet určený v parametru *strategy*. Zdůrazněme, že parametry pro vstupní a výstupní sloupce se jmenují inputCol**s** a outputCol**s**. Tj. není možné předat jen jeden sloupce v podobě stringu, nýbrž v imputovacím konstrukoru se musí nacházet listy se seznamem jmen sloupců.  \n",
    "Všimněme si též alternativního zápisu *fit* a *transform* metod - zde jsou napojeny za sebou. Jde o i jinde, nicméně z hlediska konzistence našeho textu budeme i nadále používat víceřádkový zápis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40742d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|                name|mfr|type|calories|protein|fat|sodium|fiber|carbo|sugars|potass|vitamins|shelf|weight|cups|   rating|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "|           100% Bran|  N|   C|      70|      4|  1|   130| 10.0|  5.0|     6|   280|      25|    3|   1.0|0.33|68.402973|\n",
      "|   100% Natural Bran|  Q|   C|     120|      3|  5|    15|  2.0|  8.0|     8|   135|       0|    3|   1.0| 1.0|33.983679|\n",
      "|            All-Bran|  K|   C|      70|      4|  1|   260|  9.0|  7.0|     5|   320|      25|    3|   1.0|0.33|59.425505|\n",
      "|All-Bran with Ext...|  K|   C|      50|      4|  0|   140| 14.0|  8.0|     0|   330|      25|    3|   1.0| 0.5|93.704912|\n",
      "|      Almond Delight|  R|   C|     110|      2|  2|   200|  1.0| 14.0|     8|    98|      25|    3|   1.0|0.75|34.384843|\n",
      "+--------------------+---+----+--------+-------+---+------+-----+-----+------+------+--------+-----+------+----+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=[\"carbo\", \"sugars\", \"potass\"], \n",
    "    outputCols=[\"carbo\", \"sugars\", \"potass\"],\n",
    "    strategy=\"mean\",\n",
    "    missingValue=-1\n",
    ")\n",
    "cereal_frame = imputer.fit(cereal_frame).transform(cereal_frame)\n",
    "cereal_frame.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a439331",
   "metadata": {},
   "source": [
    "Bohužel ani v PySparku 3 není možné provádět [imputování kategorických veličin](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html). Tj. textové řetězce jsou mimo hru. Nezbývá tak než použít regulárních výrazů. Například kdybychom chtěli nahradit libovolný počet mezer písmenem X, použijeme následující kód:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a236231d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------+\n",
      "|        text|   spaces|without_spaces|\n",
      "+------------+---------+--------------+\n",
      "|      normal|something|     something|\n",
      "|        zero|         |              |\n",
      "|         one|         |             X|\n",
      "|         two|         |             X|\n",
      "|       three|         |             X|\n",
      "|normal again| anything|      anything|\n",
      "+------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# works optimally for PySpark 2\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "space_dataframe_pandas = pd.DataFrame({\n",
    "    \"text\": [\"normal\", \"zero\", \"one\", \"two\", \"three\", \"normal again\"],\n",
    "    \"spaces\": [\"something\", \"\", \" \", \"  \", \"   \", \"anything\"]    \n",
    "})\n",
    "space_dataframe_spark = spark.createDataFrame(space_dataframe_pandas)\n",
    "\n",
    "space_dataframe_spark = space_dataframe_spark.withColumn(\n",
    "    \"without_spaces\",\n",
    "    f.regexp_replace(f.col(\"spaces\"), \"^\\s*$\", \"X\")\n",
    ")\n",
    "\n",
    "space_dataframe_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8bda72",
   "metadata": {},
   "source": [
    "Výše uvedený kód funguje v PySparku 2 podle očekávání. V PySparku 3 se ale zjevně něco změnilo v definici regulárních výrazů. Prázdný řetězec tak musíme ošetřit separátně pomocí when-otherwise konstrukce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2592810e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------+\n",
      "|        text|   spaces|without_spaces|\n",
      "+------------+---------+--------------+\n",
      "|      normal|something|     something|\n",
      "|        zero|         |             X|\n",
      "|         one|         |             X|\n",
      "|         two|         |             X|\n",
      "|       three|         |             X|\n",
      "|normal again| anything|      anything|\n",
      "+------------+---------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for PySpark 3\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "space_dataframe_pandas = pd.DataFrame({\n",
    "    \"text\": [\"normal\", \"zero\", \"one\", \"two\", \"three\", \"normal again\"],\n",
    "    \"spaces\": [\"something\", \"\", \" \", \"  \", \"   \", \"anything\"]    \n",
    "})\n",
    "space_dataframe_spark = spark.createDataFrame(space_dataframe_pandas)\n",
    "\n",
    "space_dataframe_spark = space_dataframe_spark.withColumn(\n",
    "    \"without_spaces\",\n",
    "    f.regexp_replace(f.col(\"spaces\"), \"^\\s*$\", \"X\")\n",
    ")\n",
    "\n",
    "space_dataframe_spark = space_dataframe_spark.withColumn(\n",
    "    \"without_spaces\", f.when(f.col(\"without_spaces\")==\"\", \"X\").otherwise(f.col(\"without_spaces\"))\n",
    ")\n",
    "\n",
    "space_dataframe_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc2c0a",
   "metadata": {},
   "source": [
    "### One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91187dbc",
   "metadata": {},
   "source": [
    "Pokračujme dál v přípravě modelu. Rozdělme si napřed data na trénovací a testovací sadu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050340be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cereal_train, cereal_test = cereal_frame.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b393fa5b",
   "metadata": {},
   "source": [
    "Oproti kosatcovému modelu zde budeme muset provést one-hot encoding. Ten v PySparku ale vyžaduje, aby byly třídy již ze stringové podoby převedeny na posloupnost integerů. Musíme tudíž na sloupce s kategorickými proměnnými vypustit StringIndexer. Pokud bychom měli jen jeden kategorický sloupec, mohli bychom psát následující: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43fb3a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|mfr|mfr_index|\n",
      "+---+---------+\n",
      "|  N|      3.0|\n",
      "|  Q|      5.0|\n",
      "|  K|      2.0|\n",
      "|  R|      6.0|\n",
      "|  G|      1.0|\n",
      "|  G|      1.0|\n",
      "|  P|      4.0|\n",
      "|  Q|      5.0|\n",
      "|  G|      1.0|\n",
      "|  G|      1.0|\n",
      "+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "string_indexer_mfr = StringIndexer(\n",
    "    inputCol=\"mfr\", \n",
    "    outputCol=\"mfr_index\", \n",
    "    stringOrderType=\"alphabetAsc\"\n",
    ")\n",
    "string_indexer_mfr_fit = string_indexer_mfr.fit(cereal_train)\n",
    "cereal_train_indexer = string_indexer_mfr_fit.transform(cereal_train)\n",
    "cereal_train_indexer.select(\"mfr\", \"mfr_index\").show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c95e5ae",
   "metadata": {},
   "source": [
    "Nicméně kategorických sloupců máme několik a tak namísto parametrů *inputCol* a *outputCol* musíme sáhnout po *inputCols* a *outputCols*. V dvojkovém PySparku tato možnost nebyla, což pozdější kroky poněkud komplikovalo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42cfd389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+-----+---------+----------+--------------+-----------+\n",
      "|mfr|type|vitamins|shelf|mfr_index|type_index|vitamins_index|shelf_index|\n",
      "+---+----+--------+-----+---------+----------+--------------+-----------+\n",
      "|  N|   C|      25|    3|      3.0|       0.0|           2.0|        2.0|\n",
      "|  Q|   C|       0|    3|      5.0|       0.0|           0.0|        2.0|\n",
      "|  K|   C|      25|    3|      2.0|       0.0|           2.0|        2.0|\n",
      "|  R|   C|      25|    3|      6.0|       0.0|           2.0|        2.0|\n",
      "|  G|   C|      25|    1|      1.0|       0.0|           2.0|        0.0|\n",
      "|  G|   C|      25|    3|      1.0|       0.0|           2.0|        2.0|\n",
      "|  P|   C|      25|    3|      4.0|       0.0|           2.0|        2.0|\n",
      "|  Q|   C|      25|    2|      5.0|       0.0|           2.0|        1.0|\n",
      "|  G|   C|      25|    1|      1.0|       0.0|           2.0|        0.0|\n",
      "|  G|   C|      25|    2|      1.0|       0.0|           2.0|        1.0|\n",
      "+---+----+--------+-----+---------+----------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only PySpark 3\n",
    "cols_for_one_hot = [\"mfr\", \"type\", \"vitamins\", \"shelf\"]\n",
    "\n",
    "string_indexer_mfr = StringIndexer(\n",
    "    inputCols=cols_for_one_hot, \n",
    "    outputCols=[column + \"_index\" for column in cols_for_one_hot], \n",
    "    stringOrderType=\"alphabetAsc\"\n",
    ")\n",
    "string_indexer_mfr_fit = string_indexer_mfr.fit(cereal_train)\n",
    "cereal_train_indexer = string_indexer_mfr_fit.transform(cereal_train)\n",
    "cereal_train_indexer.select(\n",
    "    \"mfr\", \"type\", \"vitamins\", \"shelf\",\n",
    "    \"mfr_index\", \"type_index\", \"vitamins_index\", \"shelf_index\"\n",
    ").show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91d197c",
   "metadata": {},
   "source": [
    "Na výstup *StringIndexeru* se posléze napojí *OneHotEncoder*. Zde ukazujeme příklad pro jeden sloupec. A co vlastně v novém sloupci vidíme? Pro jeho pochopení si musíme uvědomit, že se jedná o sparse vektor. První číslo značí počet tříd s tím, že poslední třída je vyřazená (to jsou ty záznamy typu (6,\\[\\],\\[\\]), tj. například pro pět tříd bychom zde viděli čtyřku. Druhé číslo označuje aktuální třídu alias index, na kterém se nachází třetí číslo - příznak přítomnosti třídy v podobě 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24589333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+\n",
      "|mfr|mfr_index|  mfr_encoded|\n",
      "+---+---------+-------------+\n",
      "|  N|      3.0|(6,[3],[1.0])|\n",
      "|  Q|      5.0|(6,[5],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|\n",
      "|  R|      6.0|    (6,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  P|      4.0|(6,[4],[1.0])|\n",
      "|  Q|      5.0|(6,[5],[1.0])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  R|      6.0|    (6,[],[])|\n",
      "|  K|      2.0|(6,[2],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  N|      3.0|(6,[3],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|\n",
      "|  G|      1.0|(6,[1],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|\n",
      "+---+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only PySpark 3\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"mfr_index\", outputCol=\"mfr_encoded\")\n",
    "encoder_fit = encoder.fit(cereal_train_indexer)\n",
    "encoder_fit.transform(cereal_train_indexer).select(\"mfr\", \"mfr_index\", \"mfr_encoded\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33ccd3",
   "metadata": {},
   "source": [
    "Příklad uplatnění na více třídách najednou:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6a2764f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------------+--------+--------------+----------------+\n",
      "|mfr|mfr_index|  mfr_encoded|vitamins|vitamins_index|vitamins_encoded|\n",
      "+---+---------+-------------+--------+--------------+----------------+\n",
      "|  N|      3.0|(6,[3],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  Q|      5.0|(6,[5],[1.0])|       0|           0.0|   (2,[0],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  R|      6.0|    (6,[],[])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  P|      4.0|(6,[4],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  Q|      5.0|(6,[5],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  R|      6.0|    (6,[],[])|      25|           2.0|       (2,[],[])|\n",
      "|  K|      2.0|(6,[2],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  K|      2.0|(6,[2],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  N|      3.0|(6,[3],[1.0])|       0|           0.0|   (2,[0],[1.0])|\n",
      "|  K|      2.0|(6,[2],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  G|      1.0|(6,[1],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  K|      2.0|(6,[2],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "|  K|      2.0|(6,[2],[1.0])|      25|           2.0|       (2,[],[])|\n",
      "+---+---------+-------------+--------+--------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only PySpark 3\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[column + \"_index\" for column in cols_for_one_hot], \n",
    "    outputCols=[column + \"_encoded\" for column in cols_for_one_hot]\n",
    ")\n",
    "encoder_fit = encoder.fit(cereal_train_indexer)\n",
    "encoder_fit.transform(cereal_train_indexer).select(\n",
    "    \"mfr\", \"mfr_index\", \"mfr_encoded\", \n",
    "    \"vitamins\", \"vitamins_index\", \"vitamins_encoded\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1127cc",
   "metadata": {},
   "source": [
    "Nyní je čas, abychom si řekli, že one-hot encoding se mezi PySparkem 2 a 3 poněkud změnit. To, co se v trojkové verzi označuje jako *OneHotEncoder*, neslo ve verzi dvojkové jméno *OneHotEncoderEstimator*. *OneHotEncoder* tehdy též existoval, ale dokázal zpracovat pouze jeden sloupec. Navíc jak již padlo, dvojkový *StringIndexer* nemohl pracovat s více sloupci naráz. Toto omezení se pak muselo obcházet pomocí kódu podobnému tomuto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for PySpark 2\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "cols_for_one_hot = [\"mfr\", \"type\", \"vitamins\", \"shelf\"]\n",
    "cols_for_one_hot_index = [name + \"_index\" for name in cols_for_one_hot]\n",
    "cols_for_one_hot_out = [name + \"_out\" for name in cols_for_one_hot]\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in cols_for_onehot]\n",
    "encoder = OneHotEncoderEstimator(inputCols=cols_for_onehot_index, outputCols=cols_for_onehot_out)\n",
    "pipeline = Pipeline(stages=indexers+[encoder])\n",
    "pipeline_fitted = pipeline.fit(cereal_train)\n",
    "dataframe_edited = pipeline_fitted.transform(cereal_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477838b8",
   "metadata": {},
   "source": [
    "Ukažme si natrénování celého modelu s tím, že budeme opět pracovat s pipelinou. Nejprve ve variantě pro PySpark 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2234d812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|   rating|        prediction|\n",
      "+---------+------------------+\n",
      "|59.425505| 60.54358006629596|\n",
      "|33.174094| 33.12394706709574|\n",
      "|49.120253| 47.95570274543439|\n",
      "|40.400208|40.373308841998984|\n",
      "|40.448772| 41.35500976017807|\n",
      "|44.330856| 46.23330342733896|\n",
      "|28.025765| 27.70918673759257|\n",
      "|21.871292| 21.40251073220914|\n",
      "|34.139765| 33.73729220897782|\n",
      "|30.313351|30.790146911007668|\n",
      "|40.105965| 39.53195632838818|\n",
      "| 40.69232|39.877234283445205|\n",
      "|30.450843|31.506571421117194|\n",
      "|63.005645| 63.54161635528529|\n",
      "|40.560159| 40.58543925790734|\n",
      "|38.839746|38.314970495626596|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#only for PySpark 3\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "cereal_train, cereal_test = cereal_frame.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "target_col = \"rating\"\n",
    "cols_for_scaling = [\n",
    "    \"calories\", \"protein\", \"fat\", \"sodium\", \"fiber\", \n",
    "    \"carbo\", \"sugars\", \"potass\", \"weight\", \"cups\"\n",
    "]\n",
    "cols_for_one_hot = [\"mfr\", \"type\", \"vitamins\", \"shelf\"]\n",
    "cols_indexed = [col + \"_index\" for col in cols_for_one_hot]\n",
    "cols_encoded = [col + \"_out\" for col in cols_for_one_hot]\n",
    "\n",
    "assembler_col_to_scale = VectorAssembler(\n",
    "    inputCols=cols_for_scaling, \n",
    "    outputCol=\"for_scaling_cols\"\n",
    ")\n",
    "scaling_stage = StandardScaler(\n",
    "    inputCol=assembler_col_to_scale.getOutputCol(),\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "indexer = StringIndexer(\n",
    "    inputCols=cols_for_one_hot, \n",
    "    outputCols=cols_indexed\n",
    ")\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=cols_indexed, \n",
    "    outputCols=cols_encoded\n",
    ")\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=cols_encoded+[\"scaled_features\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_model = LinearRegression(\n",
    "    featuresCol=assembler_all.getOutputCol(), \n",
    "    labelCol=target_col\n",
    ")\n",
    "\n",
    "regression_pipeline = Pipeline(\n",
    "    stages=[assembler_col_to_scale, scaling_stage, indexer, encoder, assembler_all, ml_model]\n",
    ")\n",
    "\n",
    "fitted_pipeline = regression_pipeline.fit(cereal_train)\n",
    "\n",
    "prediction_train = fitted_pipeline.transform(cereal_train)\n",
    "prediction_test = fitted_pipeline.transform(cereal_test)\n",
    "\n",
    "prediction_test.select(\"rating\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc43d86",
   "metadata": {},
   "source": [
    "A následně pro PySpark 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761e64b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only for PySpark 2\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "cereal_train, cereal_test = cereal_frame.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "target_col = \"rating\"\n",
    "cols_for_scaling = [\n",
    "    \"calories\", \"protein\", \"fat\", \"sodium\", \"fiber\", \n",
    "    \"carbo\", \"sugars\", \"potass\", \"weight\", \"cups\"\n",
    "]\n",
    "cols_for_one_hot = [\"mfr\", \"type\", \"vitamins\", \"shelf\"]\n",
    "cols_indexed = [col + \"_index\" for col in cols_for_one_hot]\n",
    "cols_encoded = [col + \"_out\" for col in cols_for_one_hot]\n",
    "\n",
    "assembler_col_to_scale = VectorAssembler(\n",
    "    inputCols=cols_for_scaling, \n",
    "    outputCol=\"for_scaling_cols\"\n",
    ")\n",
    "scaling_stage = StandardScaler(\n",
    "    inputCol=assembler_col_to_scale.getOutputCol(),\n",
    "    outputCol=\"scaled_features\"\n",
    ")\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in cols_for_one_hot\n",
    "]\n",
    "encoder = OneHotEncoderEstimator(\n",
    "    inputCols=cols_indexed, \n",
    "    outputCols=cols_encoded\n",
    ")\n",
    "\n",
    "assembler_all = VectorAssembler(\n",
    "    inputCols=cols_encoded+[\"scaled_features\"], \n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "ml_model = LinearRegression(\n",
    "    featuresCol=assembler_all.getOutputCol(), \n",
    "    labelCol=target_col\n",
    ")\n",
    "\n",
    "regression_pipeline = Pipeline(\n",
    "    stages=[assembler_col_to_scale, scaling_stage] + indexers + [encoder, assembler_all, ml_model]\n",
    ")\n",
    "\n",
    "fitted_pipeline = regression_pipeline.fit(cereal_train)\n",
    "\n",
    "prediction_train = fitted_pipeline.transform(cereal_train)\n",
    "prediction_test = fitted_pipeline.transform(cereal_test)\n",
    "\n",
    "prediction_test.select(\"rating\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb29638",
   "metadata": {},
   "source": [
    "Na vyhodnocení přesnosti natrénovaného regresního modelu použijeme *RegressionEvaluator*. Jako *metricName* můžeme použít například \"r2\" či \"rmse\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39d570c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 metrics for train dataset: 0.9959503861337553\n",
      "R2 metrics for test dataset: 0.9940305796610737\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", metricName=\"r2\")\n",
    "\n",
    "print(f\"R2 metrics for train dataset: {evaluator.evaluate(prediction_train)}\") \n",
    "print(f\"R2 metrics for test dataset: {evaluator.evaluate(prediction_test)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d71a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f42a39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45014a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc452bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826ba7ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a83a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f2861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
